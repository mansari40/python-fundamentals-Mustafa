# Inference on Welfare and Value Functionals under Optimal Treatment Assignment [∗]

#### Xiaohong Chen, Zhenxiao Chen [†] [‡], and Wayne Yuan Gao [§] October 30, 2025

**Abstract**


We provide theoretical results for the estimation and inference of a class of welfare and


value functionals of the nonparametric conditional average treatment effect (CATE)


function under optimal treatment assignment, i.e., treatment is assigned to an observed


type if and only if its CATE is nonnegative. For the optimal welfare functional defined


as the average value of CATE on the subpopulation with nonnegative CATE, we estab

lish the _[√]_ ~~_n_~~ asymptotic normality of the semiparametric plug-in estimators and provide


an analytical asymptotic variance formula. For more general value functionals, we show


that the plug-in estimators are typically asymptotically normal at the 1-dimensional


nonparametric estimation rate, and we provide a consistent variance estimator based on


the sieve Riesz representer, as well as a proposed computational procedure for numer

ical integration on submanifolds. The key reason underlying the different convergence


rates for the welfare functional versus the general value functional lies in that, on the


boundary subpopulation for whom CATE is zero, the integrand vanishes for the wel

fare functional but does not for general value functionals. We demonstrate in Monte


Carlo simulations the good finite-sample performance of our estimation and inference


procedures, and conduct an empirical application of our methods on the effectiveness


of job training programs on earnings using the JTPA data set.


∗ We thank Xu Cheng, Yanqin Fan, Sukjin Han, Patrick Kline, Soonwoo Kwon, Oliver Linton, Liyang
Sun, Petra Todd, and conference participants at 2025 World Congress of the Econometric Society, 2025
California Econometrics Conference, 2025 Cowles Conference on Econometrics Celebrating Don Andrews for
helpful comments and suggestions.

  - Department of Economics and Cowles Foundation for Research in Economics, Yale University, USA,
xiaohong.chen@yale.edu. Chen thanks Cowles Foundation for research support.

  - Department of Economics, University of Pennsylvania, USA, zxchen@upenn.edu.
§ Department of Economics, University of Pennsylvania, USA, waynegao@upenn.edu.


1


**Keywords:** optimal treatment assignment, conditional average treatment effect, semi

parametric estimation and inference, regular and irregular functionals

### **1 Introduction**


In this paper, we study the estimation and inference on welfare and value functionals of a

given treatment under optimal (“first-best”) treatment assignment.
Let _D_ _i_ _∈{_ 0 _,_ 1 _}_ denote a certain binary treatment for subject _i_, ( _Y_ _i_ (0) _, Y_ _i_ (1)) denote
the potential outcomes of interest, and _Y_ _i_ := _Y_ _i_ ( _D_ _i_ ) _∈_ R denote the observed outcome.
Let _X_ _i_ _∈_ R _[d]_ denote subject _i_ ’s observable characteristics of _i_, which is distributed with

density _f_ 0 . We suppose that researchers have access to a random sample of training data
_{_ ( _D_ _i_ _, Y_ _i_ _, X_ _i_ ) _}_ _[n]_ _i_ =1 [.]
Under the standard conditional unconfoundedness assumption ( _Y_ _i_ (0) _, Y_ _i_ (1)) _⊥_ _D_ _i_ _| X_ _i_
and the overlap condition _p_ 0 ( _x_ ) := E [ _D_ _i_ _| X_ _i_ = _x_ ] _∈_ (0 _,_ 1), the conditional average treatment
effect (CATE) defined by


CATE ( _x_ ) := E [ _Y_ _i_ (1) _−_ _Y_ _i_ (0) _| X_ _i_ = _x_ ]


is identified from data by


CATE ( _x_ ) _≡_ _h_ 0 ( _x_ ) := _µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _,_ (1)


where _h_ 0 : R _[d]_ _�→_ R, and

_µ_ 0 ( _x, d_ ) := E [ _Y_ _i_ _| X_ _i_ = _x, D_ _i_ = _d_ ] (2)


is the nonparametric regression function of the outcome _Y_ _i_ on _X_ _i_ and _D_ _i_ . We maintain

the conditional unconfoundedness assumption and the overlap condition, and will thereafter

simply refer to _h_ 0 as the CATE function. We further assume that _h_ 0 belongs to a Holder

class of functions with smoothness _s >_ 1.

We consider a standard scenario where policymakers can assign treatments based on co
variates, and focus on the following two core types of welfare and value parameters. The first

type is the maximized welfare of the target population under optimal treatment assignment:


_W_ ( _h_ 0 ) := [ _h_ 0 ( _x_ )] + _f_ ( _x_ ) _dx_ (3)
�


where _f_ is the marginal density of _x_ in the target population and [ _t_ ] + := max ( _t,_ 0) is the
rectified linear unit (ReLU) function. _W_ ( _h_ 0 ) averages the CATE over the population under
the “first-best” treatment assignment rule: “treat type _x_ if and only if CATE ( _x_ ) _≥_ 0,” and is

thus often referred to as the welfare under optimal treatment assignment. We will thereafter


2


refer to _W_ ( _h_ 0 ) as the welfare functional.

Alternatively, one may also be interested in evaluating the average of a value other than

CATE over the population under optimal treatment assignment:


_V_ ( _h_ 0 ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx_ (4)
�


where _v_ 0 : R _[d]_ _�→_ R is a user-defined function that may be a utility function, a cost function,

or any economically meaningful function of the observed covariate _x_ . For example, setting

_v_ 0 ( _x_ ) = _a_ _[′]_ _x_ endows _V_ ( _h_ 0 ) with the interpretation as certain aggregate characteristics of

the treated population under optimal treatment assignment, and setting _v_ 0 _≡_ 1 implies that

_V_ ( _h_ 0 ) _≡_ P _f_ ( _h_ 0 ( _X_ _i_ ) _≥_ 0) becomes the share of the target population to be treated (i.e. with
nonnegative CATE).
In the formulation of _W_ ( _h_ 0 ) and _V_ ( _h_ 0 ) above, we take the density _f_ ( _x_ ) to be known.

This is in itself relevant in settings where the covariate density of the target population is

configured or known/estimated from other sources than the training sample used to estimated

CATE. For example, CATE may be estimated from a smaller pilot program, while the

policymakers are contemplating to implement the policy on a statewide or nation-wide basis

with a much larger population. That said, in this paper we also consider an important case

where _f_ is unknown and set to be the covariate density in the underlying population of

the training sample. In this case, _f_ does not need to be estimated, as the integral with

respect to _f_ can be naturally approximated via sample average in the training sample. This

corresponds more closely to the “empirical welfare” as considered in Kitagawa and Tetenov

(2018). We provide results for this setting as well.


In this paper, we establish inference results for the welfare and value functionals _W_ ( _h_ 0 )
and _V_ ( _h_ 0 ) with nonparametric estimated CATE _h_ 0 . The results can be summarized infor
mally as follows.

For the welfare functional _W_ ( _h_ 0 ), we establish semiparametric plug-in estimators of the

welfare functional are asymptotically normal at the parametric _[√]_ ~~_n_~~ rate and derive closed
form asymptotic variance formulas, along with consistent asymptotic variance estimators.

The key insight of the _[√]_ ~~_n_~~ rate is geometric: by the definition of the welfare functional, the

integrand _h_ 0 ( _x_ ) vanishes on the boundary of the integration _{x_ : _h_ 0 ( _x_ ) = 0 _}_, neutralizing
the non-smoothness of the indicator function 1 _{h_ 0 ( _x_ ) _≥_ 0 _}_ .
In contrast, for the value functional _V_ ( _h_ 0 ) with general weight _v_ 0 that does not vanish on
the boundary _{x_ : _h_ 0 ( _x_ ) = 0 _}_, we show that the rate of convergence is slower than _[√]_ ~~_n_~~, and

_s_
is instead given by the 1-dimensional nonparametric regression rate _n_ _[−]_ 2 _s_ +1 under appropriate

conditions. We establish asymptotic normality of the semiparametric plug-in estimator under


3


this irregular convergence rate, and provide a consistent variance estimator based on the

sieve Riesz representer. In particular, the consistent variance estimator features a Hausdorff

integral on the boundary submanifold _{x_ : _h_ 0 ( _x_ ) = 0 _}_, for which we provide a numerical

integration and differentiation procedure for the computation of submanifold integrals.

We conduct an array of Monte Carlo experiments to document the good finite-sample

accuracy of our theoretical inferential results. We show that the proposed standard error

estimators perform well in finite sample, and the corresponding confidence intervals based on

the asymptotic normality result and the standard error estimators have coverage probabilities

close to their nominal levels. These findings hold not only for the _[√]_ ~~_n_~~ -estimable welfare

functional, but also for the value functional, which is estimated at slower-than- _[√]_ ~~_n_~~ rate with

standard errors computed through numerical integration and differentiation.

We also apply our results to empirical data from the Job Training Partnership Act (JTPA)
data set. Following Kitagawa and Tetenov (2018), we take 30-month post-program earning

as the outcome variable and consider two covariates: pre-program earning and education.

We then provide empirical estimates and confidence intervals for two parameters: the welfare

under first-best treatment assignment, which is _[√]_ ~~_n_~~ estimable, and the share of population

to be treated under first-best treatment assignment, which is not _[√]_ ~~_n_~~ -estimable. As in Kita
gawa and Tetenov (2018), we also consider two different scenarios: one with the cost of the

treatment incorporated, and one without. These parameters have also been estimated in

Kitagawa and Tetenov (2018) under the label of “nonparametric plug-in rule” using kernel
first-stages, but Kitagawa and Tetenov (2018) only provide point estimates with no confidence intervals for them. We use sieve (B-spline) first-stage nonparametric estimators and
find similar results to those in Kitagawa and Tetenov (2018), and further provide informative

confidence intervals for both the welfare and the share parameters.


**Closely Related Literature**


This work is a companion paper to Chen and Gao (2025), and directly applies the general

theoretical results there to handle differentiation with respect to region of integration and

semiparametric estimation of integrals over submanifolds. Specifically, this work focuses on

the important context of treatment assignment problems, and deal with two features that

are not discussed in Chen and Gao (2025). First, CATE is defined as the difference of

two nonparametric regression functions between the treated and untreated subpopulation

(or the difference of two point evaluations of a nonparametric function with treatment status defined as an argument too), and is often estimated as the difference of two first-stage

nonparametric estimators. Second, in treatment assignment problems, researchers may face

two different scenario in terms of the distribution of the covariates: sometimes, this dis

4


tribution can be treated as known and may be different from the covariate distribution in

the experimental/observational population from which CATE is estimated; in other times,

one may want to treat the covariate distribution as unknown and the same as the experi
mental/observational population, and uses sample averages to automatically incorporate the

covariate distribution. In this paper, we takes into account these special structures of the

problem, and establish the inference results by providing lower-level sufficient conditions to
the general theory in Chen and Gao (2025). [1]


Our paper makes new contributions to the literature on estimation and inference on a

general value functional of a policy under optimal treatment assignment. To the best of

our knowleadge, all the existing work on limiting distributions of functionals of a policy

under optimal treatment assignments considered _[√]_ ~~_n_~~ -normality only. Our paper is the first

to establish slower-than-root-n limiting distribution and inference results for irregular value

functionals of a policy under optimal treatment assignment. Previously, Bhattacharya and

Dupas (2012) establishes _[√]_ ~~_n_~~ -normality of the optimal welfare value under budget constraint,

using Nadaraya-Waston kernel estimator for first-stage estimation of CATE. The important

work of Kitagawa and Tetenov (2018) focuses on a related but slightly different topic: em
pirical welfare maximization within a constrained class of policy rules. That said, Kitagawa

and Tetenov (2018) also considers and reports empirical estimates on the nonparametric

plug-in rule, which can be interpreted as a plug-in estimator of the first-best welfare, though

there were no theoretical results or confidence intervals for this estimate. The recent papers

by Park (2025) and Whitehouse, Austern and Syrgkanis (2025) use soft-max functions to

smooth the max function, and apply the debiased machine learning approach to establish

asymptotic normality and provide inferential results, and Whitehouse, Austern and Syrgka
nis (2025) establishes _[√]_ ~~_n_~~ asymptotic normality of their soft-max welfare functional. Feng,
Hong and Nekipelov (2025) analyzes binary treatment assignment under constraints and the

asymptotic property of the welfare of the policy under optimal cutoff choice, and establishes

root- _n_ asymptotic normality of the functionals. Our paper complements these existing work

in several ways. First, we clarify that the _[√]_ ~~_n_~~ -estimability of the welfare functional is due to

the fact that the integrand (CATE) by construction vanishes on the boundary of the optimally treated population _{x_ : CATE ( _x_ ) = 0 _}_, which is specific to the welfare functional but

generally not satisfied for other types of value functionals. Second, we demonstrate that the


1 Two concurrent papers by Cattaneo, Titiunik and Yu (2025a,b) also feature submanifold integrals, but
focuses on 1-dimensional cases that arise from the specific context of boundary discontinuity designs. Our
current paper also differs significantly from Cattaneo, Titiunik and Yu (2025a,b), who focus on boundary
discontinuity designs and boundary treatment effects, which are very different objects from the welfare and
value functionals considered here under first-best treatment assignments. Consequently, the submanifolds
(boundaries) in their settings are given or known based on the locations or a distance function, while in our
current project the boundary submanifold is defined by the unknown and estimated CATE function.


5


_√_ ~~_n_~~ -estimability of the welfare functional can be attained without the use of smoothing/soft
max functions. Third and most importantly, our results extend well beyond the welfare

functional, and cover generic value functionals that may be slower than _[√]_ ~~_n_~~ -estimable.


The rest of the paper is organized as follows. Section 2 lays out the main model setup, and

provides a conceptual explanation of why the welfare functional _W_ ( _h_ 0 ) can be _[√]_ ~~_n_~~ -estimable
while the value functional _V_ ( _h_ 0 ) is not in general. Section 3 then establishes the inference
results for the welfare functional _W_ ( _h_ 0 ), while Section 4 provides corresponding results for
the value function _V_ ( _h_ 0 ). We report numerical results from Monte Carlo simulations in

Section 5, and conduct an empirical illustration in Section 6. Proofs of theoretical results in

the main text are available in Appendix A.

### **2 Model Setup and Functional Derivatives**


We first introduce the standard treatment effect model, the general welfare functional of

interest and the maintained assumptions in Subsection 2.1

#### **2.1 The Model and the Parameters of Interest**


ˆ
We first state the maintained assumption in the paper. Let _h_ [ˆ] ( _x_ ) := ˆ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) be a
nonparametric estimator of the CATE function, in which ˆ _µ_ ( _x, d_ ) is a first-stage estimator

of the nonparametric regression model


_Y_ _i_ = _µ_ 0 ( _X_ _i_ _, D_ _i_ ) + _ϵ_ _i_ _,_ E [ _ϵ_ _i_ _| X_ _i_ _, D_ _i_ ] = 0 _,_ E � _ϵ_ [2] _i_ ��� _X_ _i_ _, D_ _i_ � _< ∞._ (5)


We impose the following basic assumptions in this paper.


**Assumption 1** (Model) **.** _The training data and the model satisfy:_


_(a) Training sample: the training data {_ ( _Y_ _i_ _, D_ _i_ _, X_ _i_ ) _}_ _[n]_ _i_ =1 _[is a random sample drawn from]_
( _Y, D, X_ ) _∈_ R _× {_ 0 _,_ 1 _} × X satisfying Model_ (18) _, where X is a bounded rectangular_
_set (say_ [0 _,_ 1] _[d]_ _) in_ R _[d]_ _, and X_ _i_ _has its true unknown marginal density f_ 0 _supported on_

_X_ _._


_(b) Overlap:_ 0 _< p_ 0 ( _x_ ) := E[ _D_ _i_ _|X_ _i_ = _x_ ] _<_ 1 _._


_(c) Smoothness of Regression Function: For d ∈{_ 0 _,_ 1 _}, µ_ 0 ( _·, d_ ) _∈_ Λ _[s]_ ( _X_ ) _with s >_ 1 _._


We first provide a heuristic overview of our theoretical analysis, and explain the key

intuition why the welfare functional _W_ ( _h_ 0 ) may be _[√]_ ~~_n_~~ -estimable (i.e. _W_ is a regular
functionial) while _V_ ( _h_ 0 ) is generally not (i.e., _V_ is a irregular functional).


6


We first introduce a more general value functional Φ ( _h_ 0 ) that nests _W_ ( _h_ 0 ) and _V_ ( _h_ 0 )

as special cases:

Φ ( _h_ 0 ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _} ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) _f_ ( _x_ ) _dx,_ (6)
�


where _ϕ_ : R _×_ R _[d]_ _→_ R is a known measurable mapping. We note that


 - Φ ( _h_ 0 ) = _W_ ( _h_ 0 ) when _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = _h_ 0 ( _x_ );


 - Φ ( _h_ 0 ) = _V_ ( _h_ 0 ) when _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = _v_ 0 ( _x_ ) with _∂_ 1 _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = 0.


**Assumption 2** (Functional) **.** _The functional_ Φ _satisfies_


_(a) ϕ_ : R _×_ R _[d]_ _→_ R _is continuously differentiable with respect to its first argument._


_(b) Target Density: The target density f of X is absolutely continuous w.r.t._ _f_ 0 _with_
_uniformly bounded Radon-Nikodym derivative λ_ := _f/f_ 0 _._


_(c) Regular Level Set: h_ 0 ( _·_ ) := _µ_ 0 ( _·,_ 1) _−_ _µ_ 0 ( _·,_ 0) _satisfies ∥∇_ _x_ _h_ 0 ( _x_ ) _∥≥_ _c >_ 0 _on the level_
_set {x ∈X_ : _h_ 0 ( _x_ ) = 0 _}._

#### **2.2 Functional Derivatives via Generalized Leibniz rule**


By the standard semiparametric theory on the estimation of functionals of nonparametric

ˆ
regression functions, the asymptotic property of the semiparametric plug-in estimator Φ _h_
� �

can be analyzed via the functional derivative of Φ( _h_ ) w.r.t. _h_ 0 in the direction of _h −_ _h_ 0,
i.e., writing _h_ _t_ := _h_ 0 + _t_ ( _h −_ _h_ 0 ),



_D_ _h_ Φ ( _h_ 0 ) [ _h −_ _h_ 0 ] := _[d]_



= _[d]_

_dt_

����� _t_ =0



_._
(7)
����� _t_ =0



_dt_ [Φ (] _[h]_ _[t]_ [)]



_dt_



1 _{h_ _t_ ( _x_ ) _≥_ 0 _} ϕ_ ( _h_ _t_ ( _x_ ) _, x_ ) _f_ ( _x_ ) _dx_
�



_D_ _h_ _W_ ( _h_ 0 ) [ _h −_ _h_ 0 ] := _[d]_



= _[d]_

_dt_

����� _t_ =0



_dt_ _[W]_ [ (] _[h]_ _[t]_ [)]



_dt_




[ _h_ _t_ ( _x_ )] + _f_ ( _x_ ) _dx_ (8)
�

����� _t_ =0



_D_ _h_ _V_ ( _h_ 0 ) [ _h −_ _h_ 0 ] := _[d]_



= _[d]_

_dt_

����� _t_ =0



_dt_ _[V]_ [ (] _[h]_ _[t]_ [)]



_dt_



1 _{h_ _t_ ( _x_ ) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx_ (9)
�

����� _t_ =0



The presence of the ReLU/max function [ _t_ ] + in _D_ _h_ _W_ ( _h_ 0 ) and the indicator function 1 _{t ≥_ 0 _}_
induces a point of nonsmoothness at _t_ = 0, where [ _t_ ] + is nondifferentiable and 1 _{t ≥_ 0 _}_ is
discontinuous. This complicates the calculation of the functional derivatives in (8) and (9),

though to different degrees.

We now provide an overview of the key difference between the welfare and value functions

from the perspective of the generalized Leibniz rule, which has the following generic form

regarding the total time derivative of integrals with changing integrand and changing region


7


of integration:
_d_

_dt_



_G_ _t_ ( _x_ ) _dx._ (10)

� Ω _t_



The generalized Leibniz rule, [2] states that, under mild regularity conditions,



+ _⟨_ **n** _t_ ( _x_ ) _,_ **v** _t_ ( _x_ ) _⟩_ _G_ _t_ ( _x_ ) _dS_ _∂_ Ω _t_ ( _x_ )
� _∂_ Ω _t_

~~�~~ ~~�~~ � ~~�~~
( **II** )



_d_

_dt_



_G_ _t_ ( _x_ ) _dx_ =
Ω _t_ �



�



� Ω _t_ _∂t∂_ _[G]_ _[t]_ [ (] _[x]_ [)] _[ dx]_

� ~~�~~ � ~~�~~
( **I** )



(11)



where term (I) captures the effect of the change in the integrand _G_ _t_ ( _x_ ) with the region of
integration Ω _t_ held fixed, while term (II) captures the effect of the change in the region of
integration Ω _t_ with the integrand _G_ _t_ ( _x_ ) held fixed. The somewhat “nonstandard” term (II)
warrants some more explanations: _∂_ Ω _t_ denotes the boundary of Ω _t_, **n** _t_ ( _x_ ) is the outwardpointing unit normal vector, **v** _t_ ( _x_ ) is the velocity vector associated with the time movement
in the _∂_ Ω _t_, and _S_ _∂_ Ω _t_ ( _x_ ) denotes the surface measure on the boundary _∂_ Ω _t_ . Note that, when
_x_ is one-dimensional and Ω _t_ = [ _a_ _t_ _, b_ _t_ ], (11) specializes to the standard Leibniz rule:



_b_ _t_ _∂_ _[d]_

_a_ _t_ _∂t_ _[G]_ _[t]_ [ (] _[x]_ [)] _[ dx]_ [ +] _[ G]_ _[t]_ [ (] _[b]_ _[t]_ [)]




_[d]_ _[d]_

_dt_ _[b]_ _[t]_ _[ −]_ _[G]_ _[t]_ [ (] _[a]_ _[t]_ [)]



_dt_ _[a]_ _[t]_ _[.]_



_d_

_dt_



� _ab_ _t_



_b_ _t_ _b_ _t_

_G_ _t_ ( _x_ ) _dx_ =
_a_ _t_ � _a_ _t_



We observe that _D_ _h_ Φ ( _h_ 0 ), _D_ _h_ _W_ ( _h_ 0 ) and _D_ _h_ _V_ ( _h_ 0 ) are all of the form (10), with the

same parametrized region of integration


Ω _t_ := � _x ∈_ R _[d]_ : _h_ _t_ ( _x_ ) _≥_ 0 � _,_ Ω 0 := � _x ∈_ R _[d]_ : _h_ 0 ( _x_ ) _≥_ 0 �

and _∂_ Ω 0 = � _x ∈_ R _[d]_ : _h_ 0 ( _x_ ) = 0 � under mild regularity conditions on _h_ 0 .
Applying the generalized Leibniz rule (11) to the functional Φ( _h_ ), we obtain:


_D_ _h_ Φ ( _h_ 0 ) [ _h −_ _h_ 0 ] = _∂_ 1 _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) ( _h_ ( _x_ ) _−_ _h_ 0 ( _x_ )) _f_ ( _x_ ) _dx_
� Ω 0

� ~~��~~ ~~�~~
( **I** )



+ _⟨_ **n** 0 ( _x_ ) _,_ **v** 0 ( _x_ ) _⟩_ _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) _f_ ( _x_ ) _dS_ _∂_ Ω 0 ( _x_ )
� _∂_ Ω 0

~~�~~ ~~��~~ ~~�~~
( **II** )



(12)



where the first term (I) is a full-dimensional Lebesgue integral (in R _[d]_ ), while the second term
(II) is a lower-dimensional boundary integral. In particular, when _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) _f_ ( _x_ ) does not
vanish on _∂_ Ω 0 = � _x ∈_ R _[d]_ : _h_ 0 ( _x_ ) = 0 �, the second term (II) cannot be ignored, despite _∂_ Ω 0
has Lebesgue measure zero in R _[d]_ . In fact, under Assumption 2(c) (see, e.g., Chen and Gao
(2025)), the second term (II) of (12) can be expressed as


_h_ ( _x_ ) _−_ _h_ 0 ( _x_ )

_⟨_ **n** 0 ( _x_ ) _,_ **v** 0 ( _x_ ) _⟩_ _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) _f_ ( _x_ ) _dS_ _∂_ Ω 0 ( _x_ ) =

� _∂_ Ω � _∂_ Ω _h_ _x_ _[ϕ]_ [ (] _[h]_ [0] [(] _[x]_ [)] _[, x]_ [)] _[ f]_ [ (] _[x]_ [)] _[ d][H]_ _[d][−]_ [1] [ (]



_h_ ( _x_ ) _−_ _h_ 0 ( _x_ )
_∂_ Ω 0 _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_



_⟨_ **n** 0 ( _x_ ) _,_ **v** 0 ( _x_ ) _⟩_ _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) _f_ ( _x_ ) _dS_ _∂_ Ω 0 ( _x_ ) =
_∂_ Ω 0 �



_∥∇_ _x_ _h_ 0 ( _x_ ) _∥_ _[ϕ]_ [ (] _[h]_ [0] [(] _[x]_ [)] _[, x]_ [)] _[ f]_ [ (] _[x]_ [)] _[ d][H]_ _[d][−]_ [1] [ (] _[x]_ [)] _[,]_



(13)



2 See, for example, Theorem 4.2 of Delfour and Zolésio (2001).


8


where _H_ _[d][−]_ [1] denotes the ( _d −_ 1) dimensional Hausdorff measure (see Chen and Gao (2025)),
which coincides with the ( _d −_ 1) dimensional Lebesgue measure in R _[d][−]_ [1] . Thus the boundary integral term (II) of (12) is a lower-dimensional integral functional that only extracts
information about _h_ 0 on a Lebesgue measure-0 set (in R _[d]_ ).
For the welfare function Φ( _h_ 0 ) = _W_ ( _h_ 0 ), plugging _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = _h_ 0 ( _x_ ) into (12) yields


_D_ _h_ _W_ ( _h_ 0 ) [ _h −_ _h_ 0 ] = 1 _{h_ 0 ( _x_ ) _≥_ 0 _}_ [ _h_ ( _x_ ) _−_ _h_ 0 ( _x_ )] _f_ ( _x_ ) _dx,_ (14)
�


where the term (II) vanishes since _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = _h_ 0 ( _x_ ) = 0 on the boundary _∂_ Ω 0, and the
term (I) is a full-dimensional Lebesgue integral functional of _h −_ _h_ 0 .
For the value function Φ( _h_ 0 ) = _V_ ( _h_ 0 ), plugging _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) = _v_ 0 ( _x_ ) with _∂_ 1 _ϕ_ ( _h_ 0 ( _x_ ) _, x_ ) =
0 into (12) and (13) yields


_̸_



( _h_ ( _x_ ) _−_ _h_ 0 ( _x_ ))
_{_ _[x][∈]_ [R] _[d]_ [:] _[h]_ 0 [(] _[x]_ [)=0] _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_


_̸_



_D_ _h_ _V_ ( _h_ 0 ) [ _h −_ _h_ 0 ] =
�


_̸_



_v_ 0 ( _x_ ) _f_ ( _x_ ) _dH_ _[d][−]_ [1] ( _x_ ) _._ (15)
_∥∇_ _x_ _h_ 0 ( _x_ ) _∥_


_̸_



which is not 0 as long as _v_ 0 ( _x_ ) _f_ ( _x_ ) does not vanish on the boundary _∂_ Ω 0 = � _x ∈_ R _[d]_ : _h_ 0 ( _x_ ) = 0 � .
For example, setting _v_ 0 ( _x_ ) _≡_ 1 yields _V_ ( _h_ 0 ) = _P_ _f_ ( _h_ 0 ( _X_ _i_ ) _≥_ 0), the share of population
with nonnegative CATE, and the boundary integral term (II) does not vanish. In fact, as
long as _v_ 0 ( _x_ ) _f_ ( _x_ ) _̸_ = 0 on the boundary _∂_ Ω 0 = � _x ∈_ R _[d]_ : _h_ 0 ( _x_ ) = 0 �, _D_ _h_ _V_ ( _h_ 0 ) is a non-zero
( _d−_ 1) dimensional integral functional that only extracts information about _h_ 0 on a Lebesgue
measure-0 set (in R _[d]_ ), akin to a point evaluation of a nonparametric estimation.

#### **2.3 Key Difference between the Welfare and Value Functionals**


Let _L_ [2] ( _f_ ) denote the Hilbert space of square integrable (against _f_ ) functions with the inner
product _⟨g, h⟩_ 2 _,f_ := � _g_ ( _x_ ) _h_ ( _x_ ) _f_ ( _x_ ) _dx_ . For the CATE function _h_ 0 _∈_ _L_ 2 ( _f_ ), it is well-known
that a linear functional _L_ [ _h −_ _h_ 0 ] is bounded (or equivalently, continuous) if and only if


_̸_



sup
_ν_ =0 _̸_ _,ν∈L_ [2] ( _f_ )



_|L_ [ _ν_ ( _·_ )] _|_ [2]

_̸_ _E_ _f_ [ _|ν_ ( _X_ ) _|_ [2] ] _[<][ ∞]_



_̸_


which is a necessary and sufficient condition for the existence of a Riesz representer _ν_ _[∗]_ _∈_
_L_ [2] ( _f_ ) such that
_L_ [ _ν_ ] = _⟨ν_ _[∗]_ _, ν⟩_ 2 _,f_ for all _ν ∈_ _L_ [2] ( _f_ )


This in turn is a necessary condition for any plug-in estimator of the linear functional

ˆ
_L_ _h −_ _h_ 0 = _ν_ _[∗]_ _,_ _h_ [ˆ] _−_ _h_ 0
� � � � 2 _,f_ [to converge to zero at a root-] _[n]_ [ rate.]

For the welfare functional, its linear directional derivative functional _L_ [ _ν_ ] = _D_ _h_ _W_ ( _h_ 0 ) [ _ν_ ]
given in (14), we immediately see that _ν_ _[∗]_ ( _x_ ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _}_ is the Riesz representer of
the linear functional _D_ _h_ _W_ ( _h_ 0 ) [ _ν_ ] in the Hilbert space _L_ [2] ( _f_ ), and that this Riesz representer


9


has bounded norm
_∥ν_ _[∗]_ _∥_ [2] := 1 [2] _{h_ 0 ( _x_ ) _≥_ 0 _} f_ ( _x_ ) _dx ≤_ 1 _,_
�


and thus, by well-known results in, say, Chen and Liao (2014), Chen, Liao and Sun (2014) and
Chen and Pouzo (2015), the linear functional _D_ _h_ _W_ ( _h_ 0 ) [ _ν_ ] is a regular (i.e., _[√]_ ~~_n_~~ -estimable)

functional under appropriate conditions.

In contrast, for the general value functional, the linear functional corresponding to its

directional derivative _D_ _h_ _V_ ( _h_ 0 ) [ _ν_ ] given in (15) does not have a well-defined Riesz representer
in the Hilbert space _L_ [2] ( _f_ ). It is well-known that, according to Lemma 3.3 of Chen and
Pouzo (2015), Consequently, the functional _V_ becomes an irregular functional that cannot

be estimated at _[√]_ ~~_n_~~ rate.

The above provides an intuitive explanation of why the welfare functional _W_ ( _h_ 0 ) is very
special relative to general types of value functionals _V_ ( _h_ 0 ) or Φ ( _h_ 0 ), and clarifies why _W_ ( _h_ 0 )

could be _[√]_ ~~_n_~~ -estimable while others generally cannot. In subsequent sections, we provide

formal conditions and theorems that establish the _[√]_ ~~_n_~~ -normality of plug-in estimators of the

welfare functional _W_ ( _h_ 0 ), as well as the slower-than- _[√]_ ~~_n_~~ asymptotic normality for the value
functional _V_ ( _h_ 0 ).


**Remark 1** (An Alternative View) **.** _We also briefly discuss alternative view of the determi-_
_nant of_ _[√]_ ~~_n_~~ _-estimability of the welfare fucntional W_ ( _h_ 0 ) _, based on the Lipchitz continuity of_
_the ReLU/max function_ [ _t_ ] + _. We note that, under mild conditions ensuring that the level set_
_{x_ : _h_ 0 ( _x_ ) = 0 _} has Lebesgue measure_ 0 _, we may interchange the order of differentiation and_
_integral based on the almost sure differentiability of_ [ _h_ _t_ ( _x_ )] + _and the dominant convergence_

_theorem:_



_D_ _h_ _W_ ( _h_ 0 ) [ _h −_ _h_ 0 ] = _[d]_

_dt_




[ _h_ _t_ ( _x_ )] + _f_ ( _x_ ) _dx_
�

����� _t_ =0



_d_

=
� _dt_ [[] _[h]_ _[t]_ [ (] _[x]_ [)]] [+]



_f_ ( _x_ ) _dx_ (16)
����� _t_ =0



= 1 _{h_ 0 ( _x_ ) _≥_ 0 _}_ ( _h_ ( _x_ ) _−_ _h_ 0 ( _x_ )) _f_ ( _x_ ) _dx,_
�


_which yields the same formula as in_ (14) _. However, for general value functional V_ ( _h_ 0 ) _, the_
_indicator function_ 1 _{t ≥_ 0 _} is no longer Lipchitz, and there is no analog of_ (16) _: the func-_
_tional derivative D_ _h_ _V_ ( _h_ 0 ) _needs to be derived using the generalized Leibniz rule as described_
_above (or its many variants or generalized forms in differential geometry and geometric mea-_
_sure theory)._


10


### **3 Estimation and Inference of the Welfare Functional**

In this section, we focus on the welfare functional defined in (3), which can also be equiva
lently written as a functional of _µ_ 0 as follows:


_W_ ( _h_ 0 ) := [ _h_ 0 ( _x_ )] + _f_ ( _x_ ) _dx_
�


_≡_ _W_ ( _µ_ 0 ) := [ _µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0)] + _f_ ( _x_ ) _dx_ (17)
�


We write out the two equivalent definitions of the functionals based on _h_ 0 and _µ_ 0, since each

representation has its own merit. The representations _W_ ( _h_ 0 ) based on the CATE function
_h_ 0 is clearer in terms of its interpretation: the condition _h_ 0 ( _x_ ) _≥_ 0 is a direct optimal

treatment assignment, and this representation has been adopted in previous work such as

Kitagawa and Tetenov (2018). On the other hand, the representations _W_ ( _µ_ 0 ) based on _µ_ 0

is clearer in terms of the underlying nonparametric regression function, which is notationally

easier to work with in our subsequent semiparametric asymptotic analysis.

We consider two different setups for the estimation of _W_ ( _h_ 0 ), depending on how we treat
the marginal density _f_ ( _x_ ).
In the first setup, we treat _f_ as known and the functional _W_ ( _·_ ) as a known transformation
of _h_ 0 . This is relevant in cases where the covariate density _f_ ( _x_ ) of the target population is
either configured or known/estimated from other sources than the training sample used to
estimated CATE. In the formulation of _W_ ( _h_ 0 ) and _V_ ( _h_ 0 ) above, we take the density _f_ ( _x_ ) to
be known. For example, CATE may be estimated from a smaller pilot program (with sample
size _n_ ), while the policymakers are contemplating to implement the policy on a statewide or

nation-wide basis with a much larger target population, whose covariate density may either

be known at the population level or estimated from an alternative data source with much

larger sample size _N >> n_ (so that the sampling uncertainty in the estimation of _f_ becomes
negligible relative to that in the estimation of _h_ 0 ).

In the second setup, we take _f_ to be unknown and set it to _f_ 0, the covariate density in the

underlying population of the training sample. In this case, _f_ does not need to be explicitly

estimated, but the integral in _W_ ( _·_ ) with respect to _f_ can be naturally approximated via

sample average in the training sample. This corresponds more closely to the “empirical

welfare” as considered in Kitagawa and Tetenov (2018). We also provide results for this

setting as well.


11


#### 3.1 Welfare Functional Under Known Density f

We start with the first setup, where the covariate density _f_ is taken to be known and _W_ ( _·_ )

is treated as a deterministic known integral of _h_ 0 w.r.t. _f_ . In this case, we can define a

simple nonparametric plug-in estimators of _W_ ( _h_ 0 ) as



ˆ ˆ
_W_ � _h_ � _≡_ _W_ (ˆ _µ_ ) = �� _h_ ( _x_ ) �



+ _[f]_ [ (] _[x]_ [)] _[ dx.]_



ˆ
We first state some key assumptions. Let _h_ [ˆ] ( _x_ ) := ˆ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) be a nonparametric
estimator of the CATE function, in which ˆ _µ_ ( _x, d_ ) is a first-stage estimator of the nonpara
metric regression model


_Y_ _i_ = _µ_ 0 ( _X_ _i_ _, D_ _i_ ) + _ϵ_ _i_ _,_ E [ _ϵ_ _i_ _| X_ _i_ _, D_ _i_ ] = 0 _,_ E � _ϵ_ [2] _i_ ��� _X_ _i_ _, D_ _i_ � _< ∞._ (18)


ˆ
**Assumption 3.** _Fist-Stage Convergence: ∥µ −_ _µ_ 0 _∥_ _∞_ = _o_ _p_ � _n_ _[−]_ [1] _[/]_ [4] [�] _._


**Theorem 1** ( _[√]_ ~~_n_~~ -asymptotic normality for the welfare functional) **.** _Under Assumptions 1_
_and 2(b)(c), we have:_



�



_ν_ _[∗]_ ( _x, d_ ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _} λ_ ( _x_ )



_d_ 1 _−_ _d_
� _p_ 0 ( _x_ ) _[−]_ 1 _−_ _p_ 0 ( _x_ )



_is the Riesz representer for the linear functional D_ _µ_ _W_ ( _µ_ 0 ) [ _·_ ] _._

_If furthermore Assumption 3 holds, we have:_



_n_
� _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ + _o_ _p_ (1) _._

_i_ =1



_√n_ �



1
_W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) � = ~~_√n_~~



_Then:_

ˆ
_√n_ � _W_ � _h_ � _−_ _W_ ( _h_ 0 ) � _≡_ _[√]_ _n_ �


_with_


_σ_ _W_ [2] [:=][ E] � ( _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ ) [2] [�] = E


_where σ_ _ϵ_ [2] [(] _[x]_ [) :=][ E][ [] _[ϵ]_ [2] _i_ _[|][ X]_ _[i]_ [=] _[ x]_ []] _[.]_



_d_
_W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) � _−→N_ � 0 _, σ_ _W_ [2] � _,_



1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ 2 ( _X_ _i_ ) _σ_ _ϵ_ 2 [(] _[X]_ _[i]_ [)]
� _p_ 0 ( _X_ _i_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))



_,_
�



Theorem 1 suggests the following natural estimator for the asymptotic variance _σ_ _W_ [2] [:]



ˆ
_σ_ _W_ [2] [:=] _N_ [1]



�

_i_



ˆ
1 � _h_ ( _X_ _i_ ) _≥_ 0 � _λ_ [2] ( _X_ _i_ ) ˆ _u_ [2] _i_
ˆ ˆ (19)
_p_ ( _X_ _i_ ) (1 _−_ _p_ ( _X_ _i_ ))



where ˆ _u_ _i_ := _Y_ _i_ _−_ _h_ [ˆ] ( _X_ _i_ ). This requires nonparametric estimation of propensity score function
_p_ ( _x_ ), as well as the knowledge (or nonparametric estimation) of the density _f_ 0 or the RadonNikodym derivative _λ_ ( _x_ ).

Alternatively and more preferably, we can use the sieve-based asymptotic variance esti
mator, which does not require estimation or knowledge of _p_ ( _x_ ) and _λ_ ( _x_ ). To do so, we first


12


clarify some subtlety in the definition of the sieve in the first-stage nonparametric estimation

ˆ
of CATE as ˆ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0), where ˆ _µ_ is a linear sieve estimator of _µ_ 0 ( _x, d_ ) under random
design on ( _X_ _i_ _, D_ _i_ ) with _D_ _i_ being binary.
In practice, ˆ _µ_ ( _x,_ 1) is estimated with _K_ 1 linear series in the treated subsample, while
_µ_ ˆ ( _x,_ 0) is estimated separately with potentially different _K_ 0 linear series in the untreated

subsample. However, since we treat _D_ _i_ as a random variable, we cannot directly treat

ˆ
_µ_ ( _x,_ 1) and ˆ _µ_ ( _x,_ 0) as two completely separate nonparametric estimators with exogenously

given sample sizes. Instead, we treat ˆ _µ_ as the least square estimator of


_Y_ _i_ = _D_ _i_ _ψ_ [(] _[K]_ [1] [)] ( _X_ _i_ ) _′_ _β_ 1 + (1 _−_ _D_ _i_ ) _ψ_ ( _K_ 0 ) ( _X_ _i_ ) _′_ _β_ 0 + _u_ _i_ (20)



where _ψ_ [(] _[K]_ [1] [)] ( _x_ ) = ( _ψ_ 1 ( _x_ ) _, . . ., ψ_ _K_ 1 ( _x_ )) _[′]_ and _ψ_ [(] _[K]_ [0] [)] ( _x_ ) = ( _ψ_ _K_ 1 +1 ( _x_ ) _, . . ., ψ_ _K_ 1 + _K_ 0 ( _x_ )) _[′]_ denote
the B-spline basis functions used to estimate _µ_ 0 ( _x,_ 1) and _µ_ 0 ( _x,_ 0), respectively, with sieve

dimensions _K_ 1 and _K_ 0 .

Define the vector-valued function _ψ_ ( _x_ ) such that its _k_ th component equals _dψ_ _k_ ( _x_ ) for _k ∈_
_{_ 1 _, . . ., K_ 1 _}_ and (1 _−d_ ) _ψ_ _k_ ( _x_ ) for _k ∈{K_ 1 +1 _, . . ., K_ 1 + _K_ 0 _}_ . Following Chen and Christensen
(2018), specifically equations (6) and (7), the variance (or standard error) estimator in our

setting takes the form
ˆ _′_
_σ_ _W_ [2] [:=] _[ D]_ _[µ]_ _[W]_ [ (ˆ] _[µ]_ [)] � _ψ_ � ˆΩ _D_ _µ_ _W_ (ˆ _µ_ ) � _ψ_ � _,_ (21)


where Ωdenotes the estimated asymptotic covariance matrix of the OLS estimators in ( [ˆ] 20)

given by



_ψ_ � _′_ ˆΩ _D_ _µ_ _W_ (ˆ _µ_ ) �



_ψ_ � _,_ (21)



1
ˆΩ:= � Ψ [(2] _[K]_ [)] Ψ [(2] _[K]_ [)] _[′]_ [�] _[−]_ [1] _n_
�



_n_
� _u_ [2] _i_ [Ψ] [(2] _[K]_ [)] [Ψ] [(2] _[K]_ [)] _[′]_ � Ψ [(2] _[K]_ [)] Ψ [(2] _[K]_ [)] _[′]_ [�] _[−]_ [1]

_i_ =1 �



and the estimated directional derivative vector _D_ _µ_ _W_ (ˆ _µ_ ) �



_ψ_ is given by
�







 _,_



_D_ _µ_ _W_ (ˆ _µ_ ) �



=
_ψ_
�







 � _{µ_ ˆ( _x,_ 1) _−µ_ ˆ( _x,_ 0) _≥_ 0 _}_ _[ψ]_ [(] _[K]_ [1] [)] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[dx]_

 _−_ � _{µ_ ˆ( _x,_ 1) _−µ_ ˆ( _x,_ 0) _≥_ 0 _}_ _[ψ]_ [(] _[K]_ [0] [)] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[dx]_



where the minus sign in front of the sieve terms in _ψ_ _[K]_ [0] ( _x_ ) is due to the presence of the minus
sign in front of _µ_ ( _x,_ 0) in CATE( _x_ ) = _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0).

We use Sobol points to numerically compute the integral above. See the simulation

section for more details.

#### 3.2 Welfare Functional Under Unknown Density f = f 0


In certain cases, such as in Kitagawa and Tetenov (2018), one might be interested in the

welfare functional under the original distribution of covariates _F_ 0, which may not be known
or controlled. Given the random sample of ( _Y_ _i_ _, D_ _i_ _, X_ _i_ ) _[n]_ _i_ =1 [used to estimate the CATE] _[ h]_ [0] [, it]


13


is natural to use the sample mean _n_ [1] � _ni_ =1 [[] _[·]_ [] as an estimator of the population expectation]

� [ _·_ ] _dF_ 0, in which case a natural plug-in estimator of _W_ ( _h_ 0 ) _≡_ _W_ ( _µ_ 0 ) is given by



_n_



ˆ ˆ
_W_ � _h_ � _≡_ _W_ ~~[ˆ]~~ (ˆ _µ_ ) := _n_ [1]



_n_
�



_n_
� [ˆ _µ_ ( _X_ _i_ _,_ 1) _−_ _µ_ ˆ ( _X_ _i_ _,_ 0)] + _≡_ [1]

_n_

_i_ =1



+ _[.]_



_n_
�

_i_ =1



ˆ
� _h_ ( _X_ _i_ ) �



Clearly, the approximation of the integral introduces an additional source of randomness,

but the result established in the last subsection continues to be useful in this case.


**Theorem 2.** _Under Assumptions 1, 2(b)(c) and 3,_

ˆ ˆ ~~ˆ~~
_√n_ � _W_ � _h_ � _−_ _W_ ( _h_ 0 ) � _≡_ _[√]_ _n_ � _W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) �



= [1]
~~_√n_~~



_n_
�

_i_ =1



_d_
� [ _h_ 0 ( _X_ _i_ )] + _−_ _W_ ( _h_ 0 ) + _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ � + _o_ _p_ (1) _−→N_ � 0 _,_ ~~_σ_~~ [2] _W_ �



2 [�]
_where_ ~~_σ_~~ ~~[2]~~ _W_ [:=][ E] �� [ _h_ 0 ( _X_ _i_ )] + _−_ _W_ ( _h_ 0 ) + _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ � = _Var_ � [ _h_ 0 ( _X_ _i_ )] +



� + _σ_ _W_ [2] _[.]_



The standard errors can be computed similarly, based on straightforward adaptations of

the analytical formula (19) or the sieve-based formula (21) in Section 3.1.

### **4 Estimation and Inference of the Value Functional**

#### 4.1 Value Functional Under Known Density f


We now turn to the general value functional given by


_V_ ( _h_ 0 ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx_
�


_≡_ _V_ ( _µ_ 0 ) := 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx._
�


The simple plug-in estimators are defined as


ˆ ˆ
_V_ � _h_ � := � 1 � _h_ ( _x_ ) _≥_ 0 � _v_ 0 ( _x_ ) _f_ ( _x_ ) _dx_


_≡_ _V_ (ˆ _µ_ ) := 1 _{µ_ ˆ ( _x,_ 1) _−_ _µ_ ˆ ( _x,_ 0) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx._
�


Under Assumption 1 and 2(b)(c) the functional derivative of _V_ ( _µ_ 0 ) [ _ν_ ] is given by



( _ν_ ( _x,_ 1) _−_ _ν_ ( _x,_ 0))
_{_ _[x][∈]_ [R] _[d]_ [:] _[h]_ 0 [(] _[x]_ [)=0] _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_



_D_ _µ_ _V_ ( _µ_ 0 ) [ _ν_ ] :=
�



_,_ _,_

_v_ 0 ( _x_ ) _f_ ( _x_ ) _dH_ _[d][−]_ [1] ( _x_ ) _._
_∥∇_ _x_ _h_ 0 ( _x_ ) _∥_



As explained in Section 2, _V_ ( _h_ 0 ) _≡_ _V_ ( _µ_ 0 ) is generally not _[√]_ ~~_n_~~ -estimable, in particular, the
_D_ _µ_ _V_ ( _µ_ 0 ) [ _ν_ ] are not bounded (or continuous) linear functionals on _L_ [2] ( _f_ ), which means that
they do not have a Riesz representer on the whole Hilbert space _L_ [2] ( _f_ ). Nevertheless, sieve
Riesz representer is well-defined (see Chen and Gao (2025)), which will be the key ingredient


14


for the sieve variance term for the properties of _V_ (ˆ _µ_ ) _−_ _V_ ( _µ_ 0 ). In particular we need to

study the asymptotic property of the plug-in estimator of the submanifold integral of form

(15), which has been studied in Chen and Gao (2025) using linear series (Bspline) first stage:
in particular, Section 4.3 of Chen and Gao (2025) analyzes the integral on upper contour set
of the form _V_ ( _h_ 0 ) specifically. We use the result in Chen and Gao (2025) without repeating

it here, but focus on the adaptation required for the standard error computation.


**Assumption 4.** _Suppose that:_


_(a) ∥∇_ [2] _x_ _[h]_ [0] [(] _[x]_ [)] _[∥≤]_ _[M <][ ∞][.]_



_._
�



ˆ
_(b) ∥µ −_ _µ_ 0 _∥_ _∞_ _∥∇_ (ˆ _µ −_ _µ_ 0 ) _∥_ _∞_ = _o_ _p_



1
_n_ _[K]_

��



1

_d_
_n_



**Theorem 3.** _Suppose that Assumptions 1 and 2(b)(c) hold. Let_ ˆ _µ be a linear sieve estimator_
_of µ_ 0 _and suppose that Assumptions 6, 8 and 11 in Chen and Gao (2025) hold along with_

_Assumption 4 above. Then:_
_√_ ~~_n_~~ � _V_ (ˆ _µ_ ) _−_ _V_ ( _µ_ 0 ) � _d_ [2] _d_ 1



_V_ (ˆ _µ_ ) _−_ _V_ ( _µ_ 0 ) �



_d_
_−→N_ (0 _,_ 1) _,_ _with σ_ _V,n_ [2] _[≍]_ _[K]_



1

_d_
_n_



_σ_ _V,n_



The standard error estimates can be computed based on the linear sieve first stage in a

way similar to that described in Section 3.1, with the following adaptions. Again, we use

the formula

ˆ _′_
_σ_ _V_ [2] [:= ˆ] _[D]_ _[µ]_ _[V]_ [ (ˆ] _[µ]_ [)] � _ψ_ � ˆΩˆ _D_ _µ_ _V_ (ˆ _µ_ ) � _ψ_ � _,_ (22)



_ψ_ � _′_ ˆΩˆ _D_ _µ_ _V_ (ˆ _µ_ ) �



_ψ_ � _,_ (22)



where the pathwise derivative _D_ _µ_ _V_ (ˆ _µ_ ) � _ψ_ � given by



_ψ_ [(] _[K]_ [0)] ( _x_ )
_{x∈X_ : _h_ [ˆ] ( _x_ )=0 _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_ _[v]_ [0] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[ d][H]_ _[d][−]_ [1] [(] _[x]_ [)]



_ψ_ [(] _[K]_ [1)] ( _x_ )
_{x∈X_ : _h_ [ˆ] ( _x_ )=0 _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_ _[v]_ [0] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[ d][H]_ _[d][−]_ [1] [(] _[x]_ [)]



_D_ _µ_ _V_ (ˆ _µ_ ) � _ψ_ � =









_−_
�



�





 _,_



can be approximated via


ˆ
_D_ _µ_ _V_ (ˆ _µ_ ) �



=
_ψ_
�



_{x∈X_ : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [1] [)] [(] _[x]_ [)] _[v]_ [0] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[ dx]_







 21 _ϵ_ �

_−_ [1]

 2 _ϵ_



_−_ [1]



2 _ϵ_ �





_,_ (23)




_{x∈X_ : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [0] [)] [(] _[x]_ [)] _[v]_ [0] [(] _[x]_ [)] _[f]_ [(] _[x]_ [)] _[ dx]_



based on the mathematical result [3] that



1
lim
_ϵ↘_ 0 2 _ϵ_



_{x∈X_ : _−ϵ<h_ ( _x_ ) _<ϵ}_ _[ω]_ [ (] _[x]_ [)] _[ dx]_ [ =] �



�



_ω_ ( _x_ )
_{x∈X_ : _h_ ( _x_ )=0 _}_ _∥∇_ _x_ _h_ ( _x_ ) _∥_ _[d][H]_ _[d][−]_ [1] [ (] _[x]_ [)] _[ .]_



Again, we use Sobol points for numerical integration. See the simulation section for details,

as well as robustness checks with respect the choice of _ϵ_ in the numerical differentiation step.


3 See Theorem 3.13.(iii) of Evans and Gariepy (2015).


15


#### 4.2 Value Functional Under Unknown Density f = f 0

We now consider the case where _F_ = _F_ 0 and population expectation E[ _·_ ] is estimated by the
sample average _n_ [1] � _ni_ =1 [[] _[·]_ [], and seek to characterize the asymptotic behavior of the natural]

plug-in estimator of _V_ ( _h_ 0 ) _≡_ _V_ ( _µ_ 0 ) is given by



_n_


ˆ

� 1 � _h_ ( _X_ _i_ ) _≥_ 0 � _v_ 0 ( _X_ _i_ )

_i_ =1



ˆ ˆ
_V_ _h_ := [1]
� � _n_



ˆ ˆ
_V_ _h_ := [1]
� �



_n_
� 1 _{µ_ ˆ ( _X_ _i_ _,_ 1) _−_ _µ_ ˆ ( _X_ _i_ _,_ 0) _≥_ 0 _} v_ 0 ( _X_ _i_ ) _._

_i_ =1



_≡_ _V_ ~~[ˆ]~~ (ˆ _µ_ ) := [1]

_n_



_≡_ _V_ ~~[ˆ]~~ (ˆ _µ_ ) := [1]



ˆ ˆ
It turns out that the additional error in the approximation of _V_ _h_ by _V_ [ˆ] _h_ is asymptoti� � � �


ˆ
cally negligible relative to _V_ � _h_ � _−_ _V_ ( _h_ 0 ), which converges at a slower-than- _[√]_ ~~_n_~~ rate.


**Theorem 4.** _The asymptotic distribution of_ _V_ [ˆ] � _h_ ˆ � _coincides with that V_ ( _h_ [ˆ] ) _in Theorem 3._


The standard error can be computed using formula (22), with _D_ [ˆ] _µ_ _V_ (ˆ _µ_ ) � _ψ_ � given by the

following adapted estimator,



_{x∈X_ : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [1] [)] [(] _[x]_ [)] _[v]_ [0] [(] _[x]_ [) ˆ] _[f]_ [(] _[x]_ [)] _[ dx]_



ˆ
_D_ _µ_ _V_ (ˆ _µ_ ) �



=
_ψ_
�







 21 _ϵ_ �

_−_ [1]

 2 _ϵ_



_−_ [1]



2 _ϵ_ �





_,_ (24)




_{x∈X_ : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [0] [)] [(] _[x]_ [)] _[v]_ [0] [(] _[x]_ [) ˆ] _[f]_ [(] _[x]_ [)] _[ dx]_



where _f_ [ˆ] ( _x_ ) is a nonparametric density estimator of _f_ 0 ( _x_ ). We then again use Sobol points
to numerically evaluate the integral in (24).


**Remark 2.** _Even though integrals of the form_ � _w_ ( _x_ ) _f_ 0 ( _x_ ) _dx can be estimated using the sam-_

1
_ple average_ _N_ � _i_ _[w]_ [(] _[X]_ _i_ [)] _[ without the need of a nonparametric density estimator, we choose]_
_instead to estimate it using_ � _w_ ( _x_ ) ˆ _f_ ( _x_ ) _dx using the nonparametric density estimator_ ˆ _f_ ( _x_ )
_together with numerical integration based on Sobol points in_ (24) _, because we need to compute_
_the integral in_ (24) _on a small constrained domain {−ϵ <_ _h_ [ˆ] ( _x_ ) _< ϵ} for numerical differen-_
_tiation. As a result, given the empirical data_ ( _X_ _i_ ) _[N]_ _i_ =1 _[, for small choices of][ ϵ][, there might be]_

_very few, or even no, data points that fall into the small band, making the sample average_


1
_N_ � _i_ _[w]_ [(] _[X]_ _i_ [)] [1] _[{−][ϵ <]_ [ ˆ] _[h]_ [(] _[X]_ _i_ [)] _[ < ϵ][}][ very discrete (and even identically zero) for small values]_
_of ϵ. The use of a nonparametric density estimator_ _f_ [ˆ] ( _x_ ) _, along with numerical integration,_

_effectively smooths out such discreteness and results in much better numerical approximation_

_of the derivative._


16


### **5 Simulations**

#### **5.1 Results for Theorem 1**

We first report the finite-sample performance of the semiparametric estimator, its associated

standard error estimator, and the resulting confidence interval, based on the theoretical

results in Theorem 1, which concern the welfare functional under a known target distribution.

The model specifications used in the simulations are summarized in Table 1. For each

specification, random samples of size _n_ are drawn with covariates _X_ _i_ _∼_ _F_ 0, and treatment

status is assigned according to the propensity score function _p_ 0 ( _X_ _i_ ). Outcomes are then
generated as _Y_ _i_ = _µ_ 0 ( _X_ _i_ _, D_ _i_ ) + _ϵ_ _i_, with _ϵ_ _i_ _∼_ _N_ (0 _,_ 1).


Table 1: Theorem 1: Model Specifications


Model _F_ 0 _F_ _µ_ 0 ( _x, d_ ) _p_ 0 ( _x_ ) _σ_ [2]



M1 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] U[0 _,_ 1] 5 sin(2+ _d_ ( _πx−_ 0) cos(2 _._ 4 + 2 _πxx_ [2] ))



1 1
1+ _e_ _[−]_ [(1] _[−]_ [2] _[x]_ [)]



M2 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] U[0 _,_ 1] 0 _._ 5 _|x|_ + _d_ (0 _._ 5 _−_ _x_ [2] ) 1+ _e_ _[−]_ [(] _[−]_ 1 [0] _[.]_ [5+] _[x]_ [)] 1

M3 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] U[0 _,_ 1] _x_ [2] + _d_ (1 _−_ _x_ ) 1+ _e_ _[−]_ [(0] 1 _[.]_ [5] _[−][x]_ [)] 1



M4 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] [2] U[0 _,_ 1] [2] (1 _−_ _x_ [2] 1 _[−]_ _[x]_ [2] 2 [)(4 + sin] _[ x]_ [1] _[x]_ [2] [ + cos] _[ x]_ [2] [)]

+ _d_ (0 _._ 5 _x_ 1 _−_ 0 _._ 4 _x_ 2 )


M5 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] [2] U[0 _,_ 1] [2] (1 _−_ _x_ 1 _x_ 2 )(3 + sin( _πx_ 1 ) cos( _πx_ 2 ))

+ _d_ (0 _._ 3 _x_ 1 _−_ 0 _._ 3 _x_ 2 )



1 1
1+ _e_ _[−]_ [(] _[x]_ [1] _[−][x]_ [2)]


1 1
1+ _e_ _[−]_ [(] _[x]_ [1] _[−][x]_ [2)]



M6 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] [2] U[0 _,_ 1] [2] log(1 + _x_ 1 + _x_ 2 ) + _d_ ( _x_ 1 _−_ 0 _._ 7 _x_ 2 ) 1+ _e_ _[−]_ [(1] _[.]_ [5] 1 _[x]_ [1] _[−]_ [0] _[.]_ [5] _[x]_ [2)] 1



M7 U[ _−_ 0 _._ 2 _,_ 1 _._ 2] [2] U[0 _,_ 1] [2] ( _x_ [2] 1 [+] _[ x]_ [2] 2 [)] _[e]_ _[−]_ [(] _[x]_ [1] [+] _[x]_ [2] [)]

+ _d_ (0 _._ 5 _−_ _x_ 2 )



1 1
1+ _e_ _[−]_ [(] _[−]_ [0] _[.]_ [5+] _[x]_ [1+2] _[x]_ [2)]



The semiparametric estimator of the welfare functional is constructed in two steps. In

the first step, _µ_ 0 ( _x,_ 1) and _µ_ 0 ( _x,_ 0) are estimated separately for the treated group ( _D_ _i_ =
1) and the control group ( _D_ _i_ = 0) using B-spline regressions. In the second step, the

welfare functional is approximated by numerically integrating over _M_ = 5 _,_ 000 Sobol points
_{X_ _j_ _[Sobol]_ _}_ _[M]_ _j_ =1 [drawn from the target distribution] _[ F]_ [:]



~~ˆ~~ 1
_W_ (ˆ _µ_ ) =
_M_



_M_
�

_j_ =1



� _µ_ ˆ( _X_ _j_ _[Sobol]_ _,_ 1) _−_ _µ_ ˆ( _X_ _j_ _[Sobol]_ _,_ 0) � + _[,]_



where ˆ _µ_ ( _x,_ 1) and ˆ _µ_ ( _x,_ 0) denote the first-stage nonparametric estimators.

ˆ
Let _h_ [ˆ] ( _x_ ) = ˆ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) denote the estimated CATE function, and let ˆ _p_ ( _x_ ) be a

B-spline sieve estimator of the propensity score. The 95% confidence interval for the welfare


17


functional takes the usual form,

~~ˆ~~ ~~ˆ~~
_W_ (ˆ _µ_ ) _−_ 1 _._ 96 _[σ]_ [ˆ] _[W]_ _,_ _W_ (ˆ _µ_ ) + 1 _._ 96 _[σ]_ [ˆ] _[W]_
� ~~_√n_~~ ~~_√n_~~


with



_,_
�



ˆ
_σ_ _W_ [2] [=] _N_ [1]



�

_i_



ˆ
1 � _h_ ( _X_ _i_ ) _≥_ 0 � _λ_ [2] ( _X_ _i_ ) ( _Y_ _i_ _−_ _h_ [ˆ] ( _X_ _i_ )) [2]

_p_ ˆ ( _X_ _i_ ) (1 _−_ _p_ ˆ ( _X_ _i_ )) _._



To evaluate performance, we simulate each model 2,000 times at sample sizes _n_ =

1500 _,_ 3000 _,_ and 6000. Table 2 reports the true welfare functional ( _W_ true ), the average bias
of the estimator _W_ ~~[ˆ]~~ (ˆ _µ_ ) (Bias), its sampling standard deviation (SD), the average estimated
standard error (SE), the standard deviation of SE across iterations (SD(SE)), and the empirical coverage probability of the associated 95% confidence interval (Coverage). The results [4]


indicate that the nominal coverage rate is attained in nearly all cases, even with relatively

small samples, and improves further as sample size increases. The bias of the estimator also

becomes negligible relative to its sampling variability, and the proposed SE estimator closely

tracks the sampling standard deviation with high precision.


**Remark 3.** _To improve computational efficiency, we predetermine the sieve dimensions for_

_estimating µ_ 0 ( _x,_ 0) _and µ_ 0 ( _x,_ 1) _. For each model specification, we first generate a dataset_
( _Y_ _i_ _, X_ _i_ _, D_ _i_ ) _[n]_ _i_ =1 [=6000] _and apply an adaptation the sieve dimension selection procedure of Chen,_
_Christensen and Kankanala (2025) separately to the treated and control groups._ [5] _As demon-_
_strated in their paper, this approach ensures that the resulting estimators of µ_ 0 ( _x,_ 1) _and_
_µ_ 0 ( _x,_ 0) _converge at the fastest possible (i.e., minimax) rates in the sup-norm. The selected_

_sieve dimensions are then used in the simulation designs with n_ = 1500 _,_ 3000 _,_ 6000 _. In prin-_

_ciple, one could implement data-driven dimension selection within each simulation iteration,_

_but doing so would substantially increase computational cost._


**Remark 4.** _The construction of the_ 95% _confidence interval requires estimating the propen-_

_sity score function p_ 0 ( _x_ ) _. To this end, we again use the B-spline sieve estimator, regressing_

_treatment status on the covariates. The sieve dimension is similarly predetermined using_

_an adaptation of Chen, Christensen and Kankanala (2025) predetermined based on the full_
_dataset. A potential concern is that the fitted propensity score_ ˆ _p_ ( _x_ ) _may take values outside_

_the unit interval for some observations, which is likely to indicate a violation of the strict_

_overlap assumption. Accordingly, when computing the asymptotic standard deviation_ ˆ _σ_ _W_ _, we_

_trim observations with estimated propensity scores lying outside_ [0 _,_ 1] _._


4 Additional simulations based on GAM are presented in the Appendix B.1.
5 The adaptation selects a larger sieve dimension than that selected by the CCK procedure (and the npiv
R package) to achieve undersmoothing.


18


Table 2: Theorem 1: Simulation Results


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M1 1500 0.3857 0.0161 0.0470 0.0480 0.0033 0.9415

3000 0.3857 0.0080 0.0338 0.0337 0.0018 0.9480

6000 0.3857 0.0037 0.0229 0.0237 0.0010 0.9535


M2 1500 0.2358 0.0030 0.0484 0.0518 0.0031 0.9615

3000 0.2358 0.0028 0.0352 0.0366 0.0015 0.9555

6000 0.2358 0.0009 0.0243 0.0259 0.0007 0.9630


M3 1500 0.5001 0.0046 0.0579 0.0605 0.0025 0.9620

3000 0.5001 0.0025 0.0407 0.0430 0.0013 0.9595

6000 0.5001 0.0000 0.0293 0.0305 0.0006 0.9625


M4 1500 0.1033 0.0185 0.0478 0.0561 0.0074 0.9735

3000 0.1033 0.0088 0.0348 0.0400 0.0040 0.9745

6000 0.1033 0.0043 0.0248 0.0284 0.0021 0.9695


M5 1500 0.0499 0.0282 0.0418 0.0521 0.0093 0.9700

3000 0.0499 0.0158 0.0303 0.0369 0.0056 0.9720

6000 0.0499 0.0094 0.0227 0.0262 0.0032 0.9660


M6 1500 0.2315 0.0268 0.0559 0.0622 0.0048 0.9550

3000 0.2315 0.0123 0.0394 0.0444 0.0025 0.9655

6000 0.2315 0.0050 0.0288 0.0315 0.0014 0.9695


M7 1500 0.1250 0.0462 0.0505 0.0595 0.0073 0.9370

3000 0.1250 0.0218 0.0346 0.0406 0.0043 0.9610

6000 0.1250 0.0101 0.0245 0.0278 0.0022 0.9680


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


19


**5.1.1** **Sieve Variance Estimation**


One potential drawback of the plug-in approach to estimating the asymptotic variance of

the welfare functional is that it requires nonparametric estimation of the propensity score

function _p_ 0 ( _x_ ), which can introduce additional sampling noise and numerical instability. As
an alternative, one may employ the sieve variance estimator as in Chen and Liao (2014)
or Chen and Liao (2015), whose formula depends only on the pathwise derivative of the

welfare functional and a linear regression of the outcome variable on the sieve basis. When

the target distribution _F_ is known, the pathwise derivative of the welfare functional can be

numerically approximated using Sobol points drawn from the target population combined

with importance sampling. When the target distribution is unknown, the pathwise derivative

can instead be evaluated directly at the observed data and approximated by a sample average.

Simulation results and the empirical application based on the sieve variance estimator are

shown in the Appendix B.5.

#### **5.2 Results for Theorem 2**


We now investigate the finite-sample performance of our estimation and inference procedure

for the welfare functional in the case where the target distribution _F_ coincides with the pop
ulation distribution _F_ 0, though both remain unknown. The relevant model specifications are

presented in Table 3. In contrast to the designs considered in Section 5.1, these specifications

explicitly impose that _F_ and _F_ 0 share identical supports.


Table 3: Theorem 2: Model Specifications


Model _F_ 0 _F_ _µ_ 0 ( _x, d_ ) _p_ 0 ( _x_ ) _σ_ [2]



5 sin(2 _πx_ ) cos(2 _πx_ )
M8 U[0 _,_ 1] U[0 _,_ 1] + _d_ ( _−_ 0 _._ 4 + 2 _x_ [2] )



1 1
1+ _e_ _[−]_ [(1] _[−]_ [2] _[x]_ [)]



1
M9 U[0 _,_ 1] U[0 _,_ 1] 0 _._ 5 _|x|_ + _d_ (0 _._ 5 _−_ _x_ [2] ) 1+ _e_ _[−]_ [(] _[−]_ [0] _[.]_ [5+] _[x]_ [)] 1


1
M10 U[0 _,_ 1] U[0 _,_ 1] _x_ [2] + _d_ (1 _−_ _x_ ) 1+ _e_ _[−]_ [(0] _[.]_ [5] _[−][x]_ [)] 1



(1 _−_ _x_ [2] 1 _[−]_ _[x]_ [2] 2 [)(4 + sin] _[ x]_ [1] _[x]_ [2] [ + cos] _[ x]_ [2] [)]
M11 U[0 _,_ 1] [2] U[0 _,_ 1] [2]

+ _d_ (0 _._ 5 _x_ 1 _−_ 0 _._ 4 _x_ 2 )


(1 _−_ _x_ 1 _x_ 2 )(3 + sin( _πx_ 1 ) cos( _πx_ 2 ))
M12 U[0 _,_ 1] [2] U[0 _,_ 1] [2]

+ _d_ (0 _._ 3 _x_ 1 _−_ 0 _._ 3 _x_ 2 )



1 1
1+ _e_ _[−]_ [(] _[x]_ [1] _[−][x]_ [2)]


1 1
1+ _e_ _[−]_ [(] _[x]_ [1] _[−][x]_ [2)]



1
M13 U[0 _,_ 1] [2] U[0 _,_ 1] [2] log(1 + _x_ 1 + _x_ 2 ) + _d_ ( _x_ 1 _−_ 0 _._ 7 _x_ 2 ) 1+ _e_ _[−]_ [(1] _[.]_ [5] _[x]_ [1] _[−]_ [0] _[.]_ [5] _[x]_ [2)] 1



( _x_ [2] 1 [+] _[ x]_ [2] 2 [)] _[e]_ _[−]_ [(] _[x]_ [1] [+] _[x]_ [2] [)]
M14 U[0 _,_ 1] [2] U[0 _,_ 1] [2]

+ _d_ (0 _._ 5 _−_ _x_ 2 )



1 1
1+ _e_ _[−]_ [(] _[−]_ [0] _[.]_ [5+] _[x]_ [1+2] _[x]_ [2)]



The plug-in estimator _W_ ~~[ˆ]~~ (ˆ _µ_ ) proposed here differs from that in Section 5.1 only in the

second step: instead of using Sobol points to approximate the integral, we take the sample


20


average of � _µ_ ˆ( _X_ _i_ _,_ 1) _−_ _µ_ ˆ( _X_ _i_ _,_ 0) �



+ [over the observed data:]



_W_ ~~ˆ~~ (ˆ _µ_ ) = [1]

_n_



_n_
�

_i_ =1



� _µ_ ˆ( _X_ _i_ _,_ 1) _−_ _µ_ ˆ( _X_ _i_ _,_ 0) �



+ _[.]_



Relative to the known- _F_ case, the asymptotic variance of _W_ ~~[ˆ]~~ ( _h_ [ˆ] ) includes an additional
component, Var([ _h_ 0 ( _X_ _i_ )] + ). We estimate the asymptotic standard deviation ~~_σ_~~ ~~[ˆ]~~ _W_ by substi
tuting the B-spline sieve estimates for the nuisance functions and replacing the population

mean with its sample analog. As before, we restrict attention to observations with estimated

propensity scores in [0 _,_ 1]. A 95% confidence interval is then given by

~~ˆ~~ ~~ˆ~~
~~ˆ~~ ~~_σ_~~ _W_ ~~ˆ~~ ~~_σ_~~ _W_
_W_ (ˆ _µ_ ) _−_ 1 _._ 96 _,_ _W_ (ˆ _µ_ ) + 1 _._ 96 _._
� ~~_√n_~~ ~~_√n_~~ �


The simulation results [6] based on 2,000 iterations with sample sizes _n_ = 1500 _,_ 3000 _,_ and

6000 are reported in Table 4. Overall, the coverage of the proposed confidence intervals

converges to the nominal 95% level as sample size increases, while the decreasing bias and

standard deviation indicate a reduction in mean squared error.


**Remark 5.** _Extrapolation bias may arise when B-spline fits are evaluated outside the support_

_of their training samples-for instance, when_ ˆ _µ_ ( _x,_ 1) _is evaluated using control group data or_

ˆ
_µ_ ( _x,_ 0) _using treated group data. To mitigate this issue, we trim observations that fall outside_

_the common support of treated and control groups, estimate the nuisance functions on the_

_trimmed sample, and then compute the welfare functional. Since only a small fraction of_

_observations are removed, this adjustment has a negligible effect on the results._

#### **5.3 Results for Theorem 3**


To assess the finite-sample properties of our estimation inference procedure for the value

functional _V_ (ˆ _µ_ ) under a known target distribution _F_, we analyze the model described in

Table 5:


Table 5: Theorem 3: Model Specification


Model _F_ 0 _F_ _µ_ 0 ( _x_ 1 _, x_ 2 _, d_ ) _ν_ 0 ( _x_ 1 _, x_ 2 ) _p_ 0 ( _x_ 1 _, x_ 2 ) _σ_ [2]


M15 U([ _−_ 2 _,_ 2] [2] ) U([ _−_ 1 _._ 5 _,_ 1 _._ 5] [2] ) _d ·_ (1 _−_ _x_ [2] 1 _[−]_ _[x]_ [2] 2 [)] _[ ·]_ [ (4 + sin(] _[x]_ [1] [)] _[x]_ [2] [ + cos(] _[x]_ [2] [))] 1 1+ _e_ _[−]_ [(] 1 _[x]_ [1] _[−][x]_ [2)] 1


Our parameter of interest is a scaled value functional under known _F_ :

_V_ ( _h_ 0 ) = 3 [2] � 1 �� 1 _−_ _x_ [2] 1 _[−]_ _[x]_ [2] 2 �� 4 + sin( _x_ 1 ) _x_ 2 + cos( _x_ 2 ) � _≥_ 0 � _dF_ ( _x_ 1 _, x_ 2 ) _,_


6 Additional simulations based on GAM are presented in the Appendix B.2.


21


Table 4: Theorem 2: Simulation Results


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M8 1500 0.3857 0.0068 0.0414 0.0423 0.0025 0.9515

3000 0.3857 0.0029 0.0296 0.0297 0.0013 0.9550

6000 0.3857 0.0006 0.0206 0.0210 0.0007 0.9575


M9 1500 0.2358 0.0042 0.0425 0.0440 0.0025 0.9605

3000 0.2358 0.0029 0.0304 0.0311 0.0012 0.9590

6000 0.2358 0.0012 0.0209 0.0221 0.0006 0.9555


M10 1500 0.5001 0.0067 0.0511 0.0515 0.0018 0.9495

3000 0.5001 0.0037 0.0357 0.0366 0.0009 0.9600

6000 0.5001 0.0013 0.0253 0.0260 0.0005 0.9600


M11 1500 0.1033 0.0307 0.0365 0.0402 0.0038 0.9245

3000 0.1033 0.0156 0.0264 0.0286 0.0021 0.9395

6000 0.1033 0.0084 0.0192 0.0203 0.0012 0.9470


M12 1500 0.0499 0.0418 0.0316 0.0373 0.0045 0.8790

3000 0.0499 0.0232 0.0229 0.0263 0.0029 0.9135

6000 0.0499 0.0126 0.0168 0.0186 0.0017 0.9355


M13 1500 0.2315 0.0177 0.0432 0.0459 0.0032 0.9565

3000 0.2315 0.0088 0.0309 0.0323 0.0014 0.9510

6000 0.2315 0.0041 0.0221 0.0229 0.0007 0.9525


M14 1500 0.1250 0.0251 0.0382 0.0421 0.0059 0.9460

3000 0.1250 0.0117 0.0258 0.0287 0.0030 0.9585

6000 0.1250 0.0054 0.0182 0.0198 0.0013 0.9595


_Notes:_ (1) _W_ true is the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn from
_F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is the
average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE across
iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


22


where the scaling factor 3 [2] is chosen so that the integral evaluates to _π_ . Equivalently, this

setup can be interpreted as a Monte Carlo experiment for estimating the area of the unit

circle by uniformly throwing darts over a 3 _×_ 3 square.
The plug-in estimator _V_ ~~[ˆ]~~ (ˆ _µ_ ) for _V_ ( _µ_ 0 ) is constructed in two steps. In the first step, we
estimate _µ_ 0 ( _x,_ 1) = _µ_ 0 ( _x_ 1 _, x_ 2 _,_ 1) and _µ_ 0 ( _x,_ 0) = _µ_ 0 ( _x_ 1 _, x_ 2 _,_ 0) separately using B-spline sieve

estimators, fitted on the treated and control groups, respectively, with sieve dimensions pre
ˆ
determined as described in subsection 5.1. In the second step, 1 _{_ (ˆ _µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0)) _≥_ 0 _}_ is

numerically integrated using 5000 Sobol points drawn from _F_ . The resulting semiparametric

two-step estimator is



~~ˆ~~ 1
_V_ (ˆ _µ_ ) =
_M_



_M_
� 1 _{µ_ ˆ( _X_ _j_ _[Sobol]_ _,_ 1) _−_ _µ_ ˆ( _X_ _j_ _[Sobol]_ _,_ 0) � _._

_j_ =1



The 95% confidence interval is then constructed as

~~ˆ~~ ~~ˆ~~
_V_ (ˆ _µ_ ) _−_ 1 _._ 96 _[σ]_ [ˆ] _[V]_ _,_ _V_ (ˆ _µ_ ) + 1 _._ 96 _[σ]_ [ˆ] _[V]_ _,_
� ~~_√n_~~ ~~_√n_~~ �


where the asymptotic variance estimate ˆ _σ_ _V_ [2] [is given by]



_ψ_ _,_
�



ˆ
_σ_ _V_ [2] [= ˆ] _[D]_ _[µ]_ _[V]_ [ (ˆ] _[µ]_ [)] �



_ψ_ � _′_ ˆΩˆ _D_ _µ_ _V_ (ˆ _µ_ ) �



with Ωbeing the estimated asymptotic covariance matrix for the OLS estimators in the [ˆ]

linear regression model 20, and



_{x∈_ [ _−_ 1 _._ 5 _,_ 1 _._ 5] [2] : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [1] [)] [(] _[x]_ [)] 3 [1] [2] _[dx]_



ˆ
_D_ _µ_ _V_ (ˆ _µ_ ) �



_ψ_ = 3 [2]
�







 21 _ϵ_ �

_−_ [1]

 2 _ϵ_



_−_ [1]



2 _ϵ_ �



 _._



_{x∈_ [ _−_ 1 _._ 5 _,_ 1 _._ 5] [2] : _−ϵ<h_ [ˆ] ( _x_ ) _<ϵ}_ _[ψ]_ [(] _[K]_ [0] [)] [(] _[x]_ [)] 3 [1] [2] _[dx]_



We set the tuning parameter _ϵ_ = 0 _._ 005 to mitigate bias in _V_ ~~[ˆ]~~ (ˆ _µ_ ) and approximate

ˆ
_D_ _µ_ _V_ (ˆ _µ_ )[ _ν_ ] using the sample average over _M_ Sobol draws from _F_ . Because draws from
_F_ are unlikely to fall within the set _{x ∈_ [ _−_ 1 _._ 5 _,_ 1 _._ 5] [2] : _−ϵ <_ _h_ [ˆ] ( _x_ ) _< ϵ}_ when _ϵ_ is small,
we use _M_ = 1 _,_ 000 _,_ 000 Sobol points to ensure accuracy. Simulation results [7] based on 2,000

iterations with sample sizes _n_ = 1500 _,_ 3000 _,_ and 6000 are reported in Table 6. The results

show that the coverage rate reaches the nominal level with relatively modest sample sizes,

even though the value functional is not _[√]_ ~~_n_~~ -estimable. Also, as sample size increases, both

bias and standard error decrease, and the plug-in estimator for the standard error provides

a close estimate of the sampling standard deviation.


7 Additional simulations under alternative model specifications, as well as robustness checks for different
values of _ϵ_, are presented in Appendix B.3.


23


Table 6: Theorem 3: Simulation Results


Model _n_ _V_ true Bias SD SE SD(SE) Coverage


M15 1500 3.1416 0.0076 0.0710 0.0711 0.0092 0.9420

3000 3.1416 0.0080 0.0486 0.0499 0.0029 0.9475

6000 3.1416 0.0062 0.0337 0.0353 0.0016 0.9490


_Notes:_ (1) _V_ true is the true value functional, equal to _π_ . (2) Bias is the average deviation from _V_ true . (3)
SD is the sampling standard deviation of the estimator. (4) SE is the average estimated asymptotic
standard error across iterations. (5) SD(SE) is the standard deviation of SE across iterations. (6) Coverage
is the empirical coverage probability of the 95% confidence interval.

### **6 Empirical Application**


We revisit the empirical application analyzed in Kitagawa and Tetenov (2018, KT18) using Job Training Parternship Act (JTPA) dataset. The JTPA study randomized whether

applicants were eligible to receive a mix of training, job-search assistance, and other ser
vices provided under the program for a period of 18 months. A detailed description of the

study and an assessment of average program effects for five major subgroups of the target

population are provided in Bloom, Orr, Bell, Cave, Doolittle, Lin and Bos (1997).

We evaluate welfare using two outcome measures, following the approach in KT18. The

first is total earnings over the 30 months following treatment assignment. The second adjusts

this measure by subtracting $774 for individuals assigned to treatment, thereby incorporat
ing program costs. These outcomes are considered from an intention-to-treat perspective,

meaning we focus on eligibility assignment rather than treatment effects among compliers.

The available covariates include applicants’ pre-program earnings, years of education, and

treatment status. Our objective is to estimate and conduct inference on the first-best welfare

functional and the optimal fraction of the population that should receive treatment.

As the first step in the estimation and inference procedure, we trim observations outside

the common support of the treated and control groups to enforce the overlap assumption and

to avoid extrapolation when applying sieve estimators. For either outcome measure, we then

estimate _µ_ 0 ( _x,_ 1) nonparametrically using B-spline sieves fitted on the treated sample, and
_µ_ 0 ( _x,_ 0) analogously on the control sample. The sieve dimensions are selected in a data-driven
manner following Chen, Christensen and Kankanala (2025). Combining these estimates on

ˆ
the common support yields the CATE estimate _h_ [ˆ] ( _x_ ) = ˆ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0). Taking sample averages of [ _h_ [ˆ] ( _x_ )] + and 1 _{h_ [ˆ] ( _x_ ) _>_ 0 _}_ over the trimmed dataset produces the estimators _W_ ~~[ˆ]~~ (ˆ _µ_ )
and _V_ ~~[ˆ]~~ (ˆ _µ_ ), respectively. Confidence intervals for _W_ ( _µ_ 0 ) and _V_ ( _µ_ 0 ) are then constructed ac


~~ˆ~~
cording to their asymptotic theories: _W_ (ˆ _µ_ ) _±_ 1 _._ 96 ~~ˆ~~ ~~_σ_~~ _W_ _/√_
�


24



~~ˆ~~
_N_ _,_ and _V_ (ˆ _µ_ ) _±_ 1 _._ 96 ˆ _σ_ _V_ _/√_
� �



_N_,
�


where _N_ denotes the sample size of the JTPA dataset.
While computing sieve estimate of ~~_σ_~~ ~~[ˆ]~~ _W_ is straightforward, computing sieve estimate of

ˆ
_σ_ _V_ involves several additional steps. Specifically, it requires a density estimate _f_ [ˆ] ( _x_ ) for the

covariates as discussed in Remark 2. To this end, we use a Gaussian kernel density estimator,

selecting the bandwidth matrix via the smoothed cross-validation method ( `Hscv()` in the
`ks` package) and applying a scaling factor of _s_ = 3 to ensure adequate smoothness in the

presence of discrete values for years of education. Furthermore, we need to specify a small

hyperparameter _ϵ_ to provide a close approximate for the pathwise derivative of the value
functional in ˆ _σ_ _V_ . We set _ϵ_ equal to a fraction _ι_ = 0 _._ 01 of the standard deviation of _h_ [ˆ] ( _x_ ) over

the trimmed dataset. Robustness checks on the tuning parameters _s_ and _ι_ are reported in

Appendix C.1, and alternative density estimation approaches are examined in Appendix C.2.

Table 7 presents our estimation and inference results alongside the corresponding findings

from KT18. Specifically, we report our estimated welfare gain and the share of individuals

to be treated, along with their confidence intervals, based on the trimmed dataset. For

comparability, we also present results obtained using the untrimmed dataset-on which KT18

conducted their analysis-with the same choice of tuning parameters. The nonparametric

plug-in estimates from KT18, which target the same welfare and share parameters as in

our analysis, serve as an empirical benchmark. In addition, the linear rule estimates with

their associated confidence intervals from KT18 are reported to assess how conservative our

nonparametric inference procedure is relative to their parametric counterparts.

Across both the trimmed and untrimmed datasets, our estimates of the welfare gain

and the optimal treatment share are broadly consistent with the nonparametric plug-in

rule estimates reported in KT18, despite methodological difference in the first stage: we

employ sieve estimators for the nuisance functions, whereas they use a Nadaraya-Watson

estimator with an Epanechnikov kernel. A key contribution of our analysis is the provision

of confidence intervals for both parameters of interest. By contrast, KT18 report confidence

intervals only for the welfare gain under parametric rules, but not for the optimal treatment

share. Although our confidence interval for the welfare gain appears wide, its length is

comparable to that under the linear rule in KT18, underscoring that our inference procedure

remains sharp even while relying on fully nonparametric methods.


25


Table 7: Estimated Welfare Gains and Share of Population to be Treated Under Nonparametric Plug-in Rule


(a) 30-Month Post-Program Earnings, No Treatment Cost


**Method** **Share Treated** **Est. Welfare Gain**



0.89 $1,519


(0.73, 1.05) ($691, $2347)


0.92 $1,459


(0.76, 1.08) ($840, $2078)



0.89
Ours (With Trimming)



($691, $2347)



0.92
Ours (Without Trimming)



($840, $2078)



0.91 $1,693


NA NA



0.91
KT18 (Nonparametric Plug-in)



NA



0.96 $1,180


NA



0.96
KT18 (Linear)



($464, $1,896)



(b) 30-Month Post-Program Earnings, $774 Cost per Treatment


**Method** **Share Treated** **Est. Welfare Gain**



0.80 $858


(0.53, 1.07)


0.85 $768


(0.65, 1.05)



0.80
Ours (With Trimming)



($152, $1564)



0.85
Ours (Without Trimming)



($190, $1347)



0.78 $996


NA NA



0.78
KT18 (Nonparametric Plug-in)



NA



0.69 404


NA



0.69
KT18 (Linear)



($-313,$1,121)



_Note:_ We present point estimates and confidence intervals based on both the trimmed and
untrimmed datasets, along with the linear rule estimates (with its confidence intervals) and the
nonparametric plug-in rule estimates from Kitagawa and Tetenov (2018).


26


### **References**

Bhattacharya, D. and Dupas, P. (2012). Inferring welfare maximizing treatment assignment under budget constraints. _Journal of Econometrics_, **167** (1), 168–196.


Bloom, H. S., Orr, L. L., Bell, S. H., Cave, G., Doolittle, F., Lin, W. and Bos,

J. M. (1997). The benefits and costs of jtpa title ii-a programs: Key findings from the
national job training partnership act study. _Journal of Human Resources_, **32** (3), 549–576.


Cattaneo, M. D., Titiunik, R. and Yu, R. R. (2025a). Estimation and in
ference in boundary discontinuity designs: Distance-based methods. _arXiv preprint_

_arXiv:2505.05670_ .


—, — and — (2025b). Estimation and inference in boundary discontinuity designs:

Location-based methods. _arXiv preprint arXiv:2505.05670_ .


Chen, X., Christensen, T. and Kankanala, S. (2025). Adaptive estimation and uni
form confidence bands for nonparametric structural functions and elasticities. _The Review_

_of Economic Studies_, **92** (1), 162–196.


—
and Christensen, T. M. (2018). Optimal sup-norm rates and uniform inference on
nonlinear functionals of nonparametric iv regression. _Quantitative Economics_, **9** (1), 39–

84.


—
and Gao, W. Y. (2025). Semiparametric learning of integral functionals on submanifolds.

_arXiv preprint arXiv:2507.12673_ .


—
and Liao, Z. (2014). Sieve m inference on irregular parameters. _Journal of Econometrics_,
**182** (1), 70–86.


— and — (2015). Sieve semiparametric two-step gmm under weak dependence. _Journal of_
_Econometrics_, **189** (1), 163–186.


—, — and Sun, Y. (2014). Sieve inference on possibly misspecified semi-nonparametric

time series models. _Journal of Econometrics_, **178**, 639–658.


—
and Pouzo, D. (2015). Sieve wald and qlr inferences on semi/nonparametric conditional
moment models. _Econometrica_, **83** (3), 1013–1079.


Delfour, M. C. and Zolésio, J.-P. (2001). _Shapes and geometries: metrics, analysis,_

_differential calculus, and optimization_ . SIAM.


27


Evans, L. C. and Gariepy, R. F. (2015). _Measure Theory and Fine Properties of Func-_

_tions_ . CRC Press.


Feng, K., Hong, H. and Nekipelov, D. (2025). Statistical inference of optimal alloca
tions i: Regularities and their implications. _arXiv preprint arXiv:2403.18248_ .


Kitagawa, T. and Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. _Econometrica_, **86** (2), 591–616.


Park, G. (2025). Debiased machine learning when nuisance parameters appear in indicator

functions. _arXiv preprint arXiv:2403.15934_ .


Schellhase, C. and Kauermann, G. (2012). Density estimation and comparison with a

penalized mixture approach. _Computational Statistics_, **27**, 757–777.


Stone, C. J., Hansen, M. H., Kooperberg, C. and Truong, Y. K. (1997). Polynomial

splines and their tensor products in extended linear modeling. _The Annals of Statistics_,

**25** (4), 1371–1425.


Whitehouse, J., Austern, M. and Syrgkanis, V. (2025). Inference on optimal policy

values and other irregular functionals via smoothing. _arXiv preprint arXiv:2507.11780_ .

### **A Main Proofs**


_Proof of Theorem 1._ For any _ν_ s.t. � _ν_ 2 ( _x_ ) _f_ ( _x_ ) _< ∞,_ write _h_ _t_ := _h_ 0 + _tν_ and consider the

functional derivative



_D_ _h_ _W_ ( _h_ 0 ) [ _ν_ ] = _[d]_



����� _t_ =0



_dt_ _[W]_ [ (] _[h]_ _[t]_ [)]



= _[d]_

_dt_




[ _h_ _t_ ( _x_ )] + _f_ ( _x_ ) _dx_ _._
�

����� _t_ =0



We first show the interchangeability of the differentiation and integration holds:



_f_ ( _x_ ) _dx._
����� _t_ =0



_d_

_dt_



_∂_

[ _h_ _t_ ( _x_ )] + _f_ ( _x_ ) _dx_ =
� � _∂t_ [[] _[h]_ _[t]_ [ (] _[x]_ [)]] [+]

����� _t_ =0



Notice that [ _h_ _t_ ( _x_ )] + is Lipchitz in _t_, we have
��� [ _h_ _t_ ( _x_ )] + _−_ [ _h_ _s_ ( _x_ )] +



_≤|ν_ ( _x_ ) _| |t −_ _s|_
���



with

_|ν_ ( _x_ ) _| f_ ( _x_ ) _dx < ∞._
�


28


Furthermore, [ _h_ _t_ ( _x_ )] + is almost everywhere differentiable in _t_ with



_∂_
_∂t_ [[] _[h]_ _[t]_ [ (] _[x]_ [)]] [+]



= 1 _{h_ 0 ( _x_ ) _≥_ 0 _} ν_ ( _x_ ) (25)
����� _t_ =0



for any _x_ s.t. _h_ 0 ( _x_ ) _̸_ = 0. Since _{x_ : _h_ ( _x_ ) = 0 _}_ has Lebesgue measure 0, we have


_P_ _f_ ( _h_ 0 ( _X_ _i_ ) = 0) = 0 _._


Hence, (25) holds a.s.- _f_ in _x_, and thus by the dominated convergence theorem, we have



_f_ ( _x_ ) _dx._
����� _t_ =0



_d_

_dt_



_∂_

[ _h_ _t_ ( _x_ )] + _f_ ( _x_ ) _dx_ =
� � _∂t_ [[] _[h]_ _[t]_ [ (] _[x]_ [)]] [+]

����� _t_ =0



= 1 _{h_ 0 ( _x_ ) _≥_ 0 _} ν_ ( _x_ ) _f_ ( _x_ ) _dx_ (26)
�


We now switch the notation from _h_ 0 to _µ_ 0 for subsequent analysis, and consider the

functional derivative of _W_ ( _µ_ 0 ) w.r.t. _µ_ in the direction of _ν_ . Note that _µ_ ( _x, d_ ) and _ν_ ( _x, d_ )
are functions of both _x_ and _d_ . Applying (26), we have


_d_
_D_ _µ_ _W_ ( _µ_ 0 ) [ _ν_ ] := _dt_ _[W]_ [ (] _[µ]_ [0] [ +] _[ tν]_ [)]

����� _t_ =0


= 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _}_ ( _ν_ ( _x,_ 1) _−_ _ν_ ( _x,_ 0)) _f_ ( _x_ ) _dx_
�


= 1 _{h_ 0 ( _x_ ) _≥_ 0 _}_ ( _ν_ ( _x,_ 1) _−_ _ν_ ( _x,_ 0)) _λ_ ( _x_ ) _f_ 0 ( _x_ ) _dx_
�


= E [ 1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ ( _X_ _i_ ) ( _ν_ ( _X_ _i_ _,_ 1) _−_ _ν_ ( _X_ _i_ _,_ 0))]



_D_ _i_ 1 _−_ _D_ _i_
� _p_ 0 ( _X_ _i_ ) _[ν]_ [ (] _[X]_ _[i]_ _[, D]_ _[i]_ [)] _[ −]_ 1 _−_ _p_ 0 ( _X_ _i_ ) _[ν]_ [ (] _[X]_ _[i]_ _[, D]_ _[i]_ [)] ��



_D_ _i_ 1 _−_ _D_ _i_
� _p_ 0 ( _X_ _i_ ) _[−]_ 1 _−_ _p_ 0 ( _X_ _i_ )



= E


= E



1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ ( _X_ _i_ )
�


1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ ( _X_ _i_ )
�



_ν_ ( _X_ _i_ _, D_ _i_ )
� �



= E [ _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ν_ ( _X_ _i_ _, D_ _i_ )] (27)


where _λ_ := _f/f_ 0, E[ _·_ ] is expectation taken with respect to the training data distribution,

and



_ν_ _[∗]_ ( _x, d_ ) := 1 _{h_ 0 ( _x_ ) _≥_ 0 _} λ_ ( _x_ )



_d_ 1 _−_ _d_

(28)

� _p_ 0 ( _x_ ) _[−]_ 1 _−_ _p_ 0 ( _x_ ) �



is the Riesz representer for the linear functional _D_ _µ_ _W_ ( _µ_ 0 ) [ _·_ ]. Since



_D_ _i_ _−_ _D_ _i_ _p_ 0 ( _X_ _i_ ) _−_ _p_ 0 ( _X_ _i_ ) + _D_ _i_ _p_ 0 ( _X_ _i_ )
� _p_ 0 ( _X_ _i_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))


29



2 [�]

_X_ _i_
� �����









E


= E













_D_ _i_ 1 _−_ _D_ _i_
� _p_ 0 ( _X_ _i_ ) _[−]_ 1 _−_ _p_ 0 ( _X_ _i_ )



2 [�] _X_ _i_ 
� ����� 


= E � ( _D_ _i_ _−_ _p_ 0 ( _X_ _i_ )) [2] [��] � _X_ _i_ �



_p_ [2] 0 [(] _[X]_ _[i]_ [) (1] _[ −]_ _[p]_ [0] [(] _[X]_ _[i]_ [))] [2]



( _D_ _i_ _−_ _p_ 0 ( _X_ _i_ )) � _X_ _i_ = _[p]_ [0] [(] _[X]_ _[i]_ [)] [(][1] _[ −]_ _[p]_ [0] [(] _[X]_ _[i]_ [))]

_p_ [2] 0 [(] _[X]_ _[i]_ [) (1] _[ −]_ _[p]_ [0] [(] _[X]_ _[i]_ [))] [2] _p_ [2] 0 [(] _[X]_ _[i]_ [) (1] _[ −]_ _[p]_ [0] [(] _[X]_ _[i]_ [))] [2]



1

=
_p_ 0 ( _X_ _i_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))


under Assumption 1(b), the Riesz representer _ν_ _[∗]_ has finite norm



_D_ _i_ 1 _−_ _D_ _i_
� _p_ 0 ( _X_ _i_ ) _[−]_ 1 _−_ _p_ 0 ( _X_ _i_ )


_< ∞_
�



� 2 [] 



_∥ν_ _[∗]_ _∥_ [2] = E


= E



 1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ [2] ( _X_ _i_ )



1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ 2 ( _X_ _i_ )
� _p_ 0 ( _X_ _i_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))



showing that _D_ _µ_ _W_ ( _µ_ 0 ) is a regular linear functional.

Next, we control the remainder term from the linearization. Specifically, note that


_D_ _h_ _W_ ( _h_ ) [ _ν_ ] = 1 _{h_ ( _x_ ) _≥_ 0 _} ν_ ( _x_ ) _f_ ( _x_ ) _dx_
�


which is exactly of the form of the generic value functional. We then have


1

_D_ [2]
_h_ _[W]_ [ (] _[h]_ [0] [) [] _[ν, u]_ [] =]
� _{_ _[x][∈]_ [R] _[d]_ [:] _[h]_ 0 [(] _[x]_ [)=0] _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_ _[ν]_ [ (] _[x]_ [)] _[ u]_ [ (] _[x]_ [)] _[ f]_ [(] _[x]_ [)] _[d][H]_ _[d][−]_ [1] [ (] _[x]_ [)]

_≤_ [1]

_ϵ_ _[∥][ν][∥]_ _[∞]_ _[∥][u][∥]_ _[∞]_ _[.]_


Similar bound applies to _D_ _µ_ [2] _[W]_ [ (] _[h]_ [0] [) [] _[ν, u]_ [] as well. Thus we obtain:]

ˆ 2
��� _W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) _−_ _D_ _µ_ _W_ ( _µ_ 0 ) [ˆ _µ −_ _µ_ 0 ] ��� _≤_ _M ∥µ −_ _µ_ 0 _∥_ _∞_ _[.]_



Hence, we have

_√n_ �



ˆ
_W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) � = _[√]_ _nD_ _µ_ _W_ ( _µ_ 0 ) [ˆ _µ −_ _µ_ 0 ] + _[√]_ _nO_ _p_ � _∥µ −_ _µ_ 0 _∥_ [2] _∞_



�



1

=
~~_√n_~~



_n_
� _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ + _o_ _p_ (1) + _[√]_ _no_ _p_ � _n_ _[−]_ [1] _[/]_ [2] [�]

_i_ =1



_d_
_−→N_ � 0 _, σ_ _W_ [2] �


where, writing _σ_ _ϵ_ [2] [(] _[x]_ [) :=][ E][ [] _[ϵ]_ [2] _i_ _[|][ X]_ _[i]_ [=] _[ x]_ [],]



�



_σ_ _W_ [2] [:= Var (] _[ν]_ _[∗]_ [(] _[X]_ _[i]_ _[, D]_ _[i]_ [)] _[ ϵ]_ _[i]_ [) =][ E]



1 _{h_ 0 ( _X_ _i_ ) _≥_ 0 _} λ_ 2 ( _X_ _i_ ) _σ_ _ϵ_ 2 [(] _[X]_ _[i]_ [)]
� _p_ 0 ( _X_ _i_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))



1 _{h_ 0 ( _x_ ) _≥_ 0 _} λ_ 2 ( _x_ ) _σ_ _ϵ_ 2 [(] _[x]_ [)]
= _f_ 0 ( _x_ ) _dx_
� _p_ 0 ( _x_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))

1 _{h_ 0 ( _x_ ) _≥_ 0 _} λ_ ( _x_ ) _σ_ _ϵ_ 2 [(] _[x]_ [)]
= _f_ ( _x_ ) _dx_
� _p_ 0 ( _x_ ) (1 _−_ _p_ 0 ( _X_ _i_ ))


_Proof of Theorem 2._ We apply the derivations in Theorem 1 with _f_ = _f_ 0 and consequently


30


_λ ≡_ 1.

Consider the following standard empircal process decomposition

_√n_ (P _n_ _g_ _µ_ _−_ _Pg_ _µ_ 0 ) = _√nP_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) + G _n_ _g_ _µ_ 0 + G _n_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) _,_ (29)


_n_

where _g_ _µ_ ( _x_ ) := [ _µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0)] +, P _n_ _g_ := _n_ [1] � _i_ =1 _[g]_ [,] _[ Pg]_ [ =] � _gdF_ 0 and G _n_ := _√_ ~~_n_~~ (P _n_ _−_ _P_ ).

Note that the term _[√]_ ~~_n_~~ _P_ ( _g_ _h_ ˆ _−_ _g_ _h_ 0 ) corresponds the analysis of population expectation (integral) with respect to the true distribution _F_ = _F_ 0 in the last subsection. There are two

additional terms that appear due to the use of the sample average: a “stochastic equicon
tinuity” term G _n_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) that will be shown to be asymptotically negligible under the

permanance of Donsker property, as well as a “CLT”-term G _n_ _g_ _µ_ 0 that adds to the asymp
totic variance of the estimator.

Under Assumption 1(c), ˆ _µ −_ _µ_ 0 is assumed to belong to a Donsker class of functions,
and the Lipchitz transformation of _µ_ 0 through the ReLU function [ _·_ ] + preserves the Donsker
property. Hence, _g_ _µ_ ˆ _−_ _g_ _µ_ 0 also belongs to a Donsker class and thus G _n_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) = _o_ _p_ (1).
Then, by (29) we have,

ˆ ˆ ~~ˆ~~
_√n_ � _W_ � _h_ � _−_ _W_ ( _h_ 0 ) � _≡_ _[√]_ _n_ � _W_ (ˆ _µ_ ) _−_ _W_ ( _µ_ 0 ) �



= [1]
~~_√n_~~



_n_
�

_i_ =1



_d_
� [ _h_ 0 ( _X_ _i_ )] + _−_ _W_ ( _h_ 0 ) + _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ � + _o_ _p_ (1) _−→N_ � 0 _,_ ~~_σ_~~ [2] _W_ �



where



~~_σ_~~ [2] _W_ [:= Var] � [ _h_ 0 ( _X_ _i_ )] + _−_ _W_ ( _h_ 0 ) + _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ �



= Var � [ _h_ 0 ( _X_ _i_ )] +

= Var � [ _h_ 0 ( _X_ _i_ )] +

= Var � [ _h_ 0 ( _X_ _i_ )] +



� + Var ( _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ ) + 2Cov � [ _h_ 0 ( _X_ _i_ )] + _, ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ �

� + Var ( _ν_ _[∗]_ ( _X_ _i_ _, D_ _i_ ) _ϵ_ _i_ )

� + _σ_ _W_ [2] _[.]_



_Proof of Theorem 3._ Recall that


ˆ ˆ
_θ_ 0 = _V_ ( _h_ 0 ) := � 1 _{h_ 0 ( _x_ ) _≥_ 0 _} v_ 0 ( _x_ ) _f_ ( _x_ ) _dx,_ _θ_ = _V_ � _h_ � = _V_ (ˆ _µ_ ) _._


Taking the functional derivative according to Chen & Gao (2025), we obtain the following

submanifold integral with submanifold dimension _m_ = _d −_ 1,


_ν_ ( _x_ )

_D_ _h_ _V_ ( _h_ 0 ) [ _ν_ ] :=
� _{_ _[x][∈]_ [R] _[d]_ [:] _[h]_ 0 [(] _[x]_ [)=0] _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_ _[v]_ [0] [ (] _[x]_ [)] _[ f]_ [ (] _[x]_ [)] _[ d][H]_ _[d][−]_ [1] [ (] _[x]_ [)] _[ .]_


31


Equivalently, using the notation _µ_ 0, we have



( _ν_ ( _x,_ 1) _−_ _ν_ ( _x,_ 0))
_{_ _[x][∈]_ [R] _[d]_ [:] _[h]_ 0 [(] _[x]_ [)=0] _}_ _∥∇_ _x_ _h_ 0 ( _x_ ) _∥_



_D_ _µ_ _V_ ( _µ_ 0 ) [ _ν_ ] :=
�



_,_ _,_

_v_ 0 ( _x_ ) _f_ ( _x_ ) _dH_ _[d][−]_ [1] ( _x_ ) _._
_∥∇_ _x_ _h_ 0 ( _x_ ) _∥_



Applying Theorem 4 and Proposition 2 in Chen and Gao (2025), we obtain that

ˆ
_√_ ~~_n_~~ � _θ −_ _θ_ 0 � _d_ [2] _d_ 1



1

_d_
_n_ _._



_σ_ _V,n_



_d_
_−→N_ (0 _,_ 1) with _σ_ _V,n_ [2] _[≍]_ _[K]_



_Proof of Theorem 4._ We now work with the following rescaled empirical process decomposi
tion

~~_n_~~ ~~_n_~~

(P _n_ _g_ _µ_ _−_ _Pg_ _µ_ 0 ) = _P_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) + _σ_ _V,n_ _[−]_ [1] [G] _[n]_ _[g]_ _[µ]_ 0 [+] _[ σ]_ _V,n_ _[−]_ [1] [G] _[n]_ [(] _[g]_ _[µ]_ [ˆ] _[−]_ _[g]_ _[µ]_ 0 [)] _[,]_ (30)
_σ_ [2] _σ_ [2]

� _V,n_ ~~�~~ _V,n_


where _g_ _µ_ ( _x_ ) := 1 _{µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) _≥_ 0 _}_ .
Note that the term _P_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) _≡_ _V_ ( _µ_ ) _−V_ ( _µ_ 0 ) has been analyzed in Section 4.1, where

we have shown that

~~_n_~~ _d_

_P_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) _−→N_ (0 _,_ 1) _._
_σ_ [2]

� _V,n_

Given the above, the term _σ_ _V,n_ _[−]_ [1] [G] _[n]_ _[g]_ _[µ]_ 0 [=] _[ O]_ _[p]_ ~~�~~ _K_ _n_ _[−]_ [1] _[/d]_ = _o_ _p_ (1) becomes asymptotically

� ~~�~~

negligible. Below we seek to show that, in fact, the last term is also asymptotically negligible:



(P _n_ _g_ _µ_ _−_ _Pg_ _µ_ 0 ) =
_σ_ [2]
_V,n_



~~_n_~~


_σ_ [2]

~~�~~



_P_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) + _σ_ _V,n_ _[−]_ [1] [G] _[n]_ _[g]_ _[µ]_ 0 [+] _[ σ]_ _V,n_ _[−]_ [1] [G] _[n]_ [(] _[g]_ _[µ]_ [ˆ] _[−]_ _[g]_ _[µ]_ 0 [)] _[,]_ (30)
_σ_ [2]
_V,n_



Given the above, the term _σ_ _V,n_ _[−]_ [1] [G] _[n]_ _[g]_ _[µ]_ 0 [=] _[ O]_ _[p]_



� ~~�~~



_K_ _n_ _[−]_ [1] _[/d]_



_σ_ _V,n_ _[−]_ [1] [G] _[n]_ [(] _[g]_ _[µ]_ [ˆ] _[−]_ _[g]_ _[µ]_ 0 [) =] _[ o]_ _[p]_ [(1)] _[ .]_


Note that _g_ _µ_ ( _x_ ) involves a discontinuous indicator function, which does not preserve
the Donsker property for the Holder class in general. [8] We thus directly derive the Donsker

property via a maximal inequality on the functional class


_G_ _a_ _n_ := _{g_ _µ_ _−_ _g_ _µ_ 0 : _∥µ −_ _µ_ 0 _∥_ _∞_ _≤_ _a_ _n_ _} ._


We first obtain an envelope function for _G_ _a_ _n_ and its second moment:


_|g_ _µ_ ( _x_ ) _−_ _g_ _µ_ 0 ( _x_ ) _|_


= _|_ 1 _{µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) _≥_ 0 _} −_ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _}|_


= 1 _{µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) _≥_ 0 _> µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _}_


+ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _> µ_ ( _x,_ 1) _−_ _µ_ ( _x,_ 0) _}_


_≤_ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) + 2 _∥µ −_ _µ_ 0 _∥_ _∞_ _≥_ 0 _> µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _}_


8 In contrast, the indicator function transformation preserves the Donsker property for VC classes of
functions, in which case the Donsker property would deliver G _n_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) = _o_ _p_ (1), which implies the
weaker condition _K_ _n_ _[−]_ [1] _[/d]_ G _n_ ( _g_ _µ_ ˆ _−_ _g_ _µ_ 0 ) = _o_ _p_ (1) required in this paper.


32


with



+ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _> µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _−_ 2 _∥µ −_ _µ_ 0 _∥_ _∞_ _}_


_≤_ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) + 2 _a_ _n_ _≥_ 0 _> µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _}_


+ 1 _{µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _≥_ 0 _> µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _−_ 2 _a_ _n_ _}_


= 1 _{|µ_ 0 ( _x,_ 1) _−_ _µ_ 0 ( _x,_ 0) _| ≤_ 2 _a_ _n_ _}_


=: _G_ _a_ _n_


_PG_ [2] _a_ _n_ [=][ P][ (] _[|][µ]_ [0] [(] _[X]_ _[i]_ _[,]_ [ 1)] _[ −]_ _[µ]_ [0] [(] _[X]_ _[i]_ _[,]_ [ 0)] _[| ≤]_ [2] _[a]_ _[n]_ [)]


= P ( _|h_ 0 ( _X_ ) _| ≤_ 2 _a_ _n_ )


_≤_ _Ma_ _n_ _._



Then, provided a finite uniform entropy integral _J_ _G_,

_P_ _∥µ−µ_ sup 0 _∥_ _∞_ _≤a_ _n_ _|_ G _n_ ( _g_ _µ_ _−_ _g_ _µ_ 0 ) _| ≤_ _J_ _G_ ~~�~~ _PG_ [2] _a_ _n_ _[≤]_ _[M]_ _[√]_ ~~_[a]_~~ _[n]_



and thus

_σ_ _V,n_ _[−]_ [1] [G] _[n]_ [(] _[g]_ _[µ]_ _[−]_ _[g]_ _[µ]_ 0 [) =] _[ O]_ _[p]_



� ~~�~~ _K_ _n_ _[−]_ [1] _[/d]_ _a_ _n_



= _o_ _p_ (1) _._
�



Hence,

~~_n_~~


_σ_ [2]

~~�~~ _V,n_



ˆ ˆ
� _V_ � _h_ � _−_ _V_ ( _h_ 0 ) � _≡_


=



~~_n_~~


_σ_ [2]

~~�~~ _V,n_



~~_n_~~


_σ_ [2]

� _V,n_



~~ˆ~~
_V_ (ˆ _µ_ ) _−_ _V_ ( _µ_ 0 )
� �



�



_V_ (ˆ _µ_ ) _−_ _V_ ( _µ_ 0 ) � + _o_ _p_ (1)



_d_
_−→N_ (0 _,_ 1) _._

### **B Additional Simulation Results**

#### **B.1 Theorem 1**


In principle, a variety of nonparametric estimators can be employed in the first step for the

nuisance parameters, provided their convergence rates are sufficiently fast. In this subsection,

we assess the estimation and inference performance of the welfare functional estimator under

a known target distribution _F_, using a generalized additive model (GAM) as the first-stage

estimator. Specifically, for Models 1-3, we use B-splines as the smooth terms in GAM, while

for Models 4-7, we adopt the default tensor product smooths with cubic regression splines.

The simulation results are presented in Table 8. In Models 6 and 7, the confidence


33


intervals exhibit overcoverage relative to the nominal 95% level. This likely reflects the

slow convergence of the variance plug-in estimator toward the true asymptotic variance of

the welfare estimator, potentially due to suboptimal choices of smooth terms in the GAM

specification.


Table 8: Theorem 1 Simulation Results for Models 1–7 (GAM)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M1 1500 0.3857 0.0062 0.0474 0.0481 0.0035 0.9500

3000 0.3857 0.0025 0.0341 0.0339 0.0018 0.9525

6000 0.3857 -0.0009 0.0231 0.0239 0.0009 0.9545


M2 1500 0.2358 -0.0092 0.0445 0.0513 0.0027 0.9670

3000 0.2358 -0.0057 0.0335 0.0365 0.0013 0.9610

6000 0.2358 -0.0048 0.0239 0.0259 0.0006 0.9615


M3 1500 0.5001 0.0037 0.0587 0.0604 0.0022 0.9580

3000 0.5001 0.0021 0.0409 0.0430 0.0011 0.9605

6000 0.5001 -0.0002 0.0294 0.0305 0.0006 0.9600


M4 1500 0.1033 0.0303 0.0465 0.0550 0.0065 0.9615

3000 0.1033 0.0193 0.0338 0.0394 0.0036 0.9635

6000 0.1033 0.0130 0.0243 0.0281 0.0021 0.9610


M5 1500 0.0499 0.0316 0.0407 0.0527 0.0087 0.9685

3000 0.0499 0.0217 0.0294 0.0372 0.0050 0.9680

6000 0.0499 0.0138 0.0218 0.0261 0.0030 0.9635


M6 1500 0.2315 0.0035 0.0450 0.0634 0.0039 0.9930

3000 0.2315 0.0004 0.0314 0.0448 0.0019 0.9965

6000 0.2315 -0.0008 0.0234 0.0317 0.0010 0.9920


M7 1500 0.1250 0.0040 0.0380 0.0545 0.0064 0.9950

3000 0.1250 0.0026 0.0272 0.0385 0.0030 0.9920

6000 0.1250 0.0016 0.0199 0.0272 0.0015 0.9915


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.

#### **B.2 Theorem 2**


We next examine the estimation and inference performance of the welfare functional esti
mator under an unknown target distribution _F_, using GAM as the first-stage estimator.


34


The GAM specifications are identical to those described in the previous subsection. The

simulation results, reported in Table 9, demonstrate that our proposed inference procedure

exhibits strong finite-sample performance.


Table 9: Theorem 2 Simulation Results for Models 1–7 (GAM)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M8 1,500 0.3857 0.0077 0.0412 0.0421 0.0025 0.951
3,000 0.3857 0.0036 0.0296 0.0297 0.0013 0.952
6,000 0.3857 0.0008 0.0206 0.0209 0.0007 0.956


M9 1,500 0.2358 0.0000 0.0414 0.0436 0.0021 0.965
3,000 0.2358 0.0008 0.0297 0.0310 0.0010 0.962
6,000 0.2358 0.0002 0.0206 0.0220 0.0005 0.960


M10 1,500 0.5001 0.0059 0.0512 0.0514 0.0018 0.952
3,000 0.5001 0.0033 0.0357 0.0366 0.0010 0.957
6,000 0.5001 0.0010 0.0252 0.0260 0.0005 0.962


M11 1,500 0.1033 0.0266 0.0362 0.0396 0.0036 0.938
3,000 0.1033 0.0154 0.0261 0.0284 0.0020 0.940
6,000 0.1033 0.0094 0.0190 0.0202 0.0011 0.942


M12 1,500 0.0499 0.0277 0.0324 0.0370 0.0052 0.941
3,000 0.0499 0.0172 0.0232 0.0263 0.0029 0.946
6,000 0.0499 0.0106 0.0169 0.0187 0.0016 0.941


M13 1,500 0.2315 0.0062 0.0431 0.0456 0.0024 0.964
3,000 0.2315 0.0027 0.0308 0.0323 0.0012 0.959
6,000 0.2315 0.0007 0.0218 0.0229 0.0006 0.957


M14 1,500 0.1250 0.0056 0.0367 0.0394 0.0043 0.957
3,000 0.1250 0.0023 0.0249 0.0276 0.0019 0.968
6,000 0.1250 0.0007 0.0176 0.0195 0.0009 0.969


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.

#### **B.3 Theorem 3**


In addition to the model specification considered in the main text for the Theorem 3 sim
ulations, we conducted two auxiliary simulation studies with slightly modified designs: one
with the target population _F ∼_ U([ _−_ 1 _._ 75 _,_ 1 _._ 75] [2] ) and the other with _F ∼_ U([ _−_ 1 _._ 9 _,_ 1 _._ 9] [2] ),

keeping all other settings fixed. The results are reported in Table 10. We find that the


35


coverage rate approaches 95% as the sample size increases in both cases. However, in the

latter case the supports of the treated and control groups do not fully nest the target pop
ulation _F_, potentially leading to extrapolation bias of the first-stage estimator and a slight

undercoverage.


Table 10: Sensitivity Checks for Model 15


Case _n_ _W_ true Bias SD SE Coverage


(a) _F ∼_ U[ _−_ 1 _._ 75 _,_ 1 _._ 75] [2] 1500 3.1416 0.0222 0.1289 0.1198 0.926
3000 3.1416 0.0077 0.0801 0.0796 0.942

6000 3.1416 0.0089 0.0542 0.0558 0.952


(b) _F ∼_ U[ _−_ 1 _._ 9 _,_ 1 _._ 9] [2] 1500 3.1416 0.0118 0.0746 0.0676 0.923
3000 3.1416 0.0047 0.0474 0.0453 0.938

6000 3.1416 0.0037 0.0334 0.0317 0.936


Besides the sieve dimensions for estimating _µ_ 0 ( _x,_ 1) and _µ_ 0 ( _x,_ 0), _ϵ_ is an additional tuning

parameter that determines how well the level set is approximated. To assess the sensitivity

of our method to this parameter, we consider the alternative values _ϵ_ = 0 _._ 0025 and _ϵ_ =

0 _._ 0075 while keeping anything else fixed. Table 11 shows that the simulation results are not

materially affected by the choice of _ϵ_ .


Table 11: Sensitivity Checks for Model 15


Case _n_ _W_ true Bias SD SE Coverage


(a) _ϵ_ = 0 _._ 0075 1500 3.1416 0.0120 0.0642 0.0635 0.9395
3000 3.1416 0.0126 0.0442 0.0448 0.9445

6000 3.1416 0.0102 0.0311 0.0316 0.9445


(b) _ϵ_ = 0 _._ 0025 1500 3.1416 0.0120 0.0642 0.0638 0.9420
3000 3.1416 0.0126 0.0442 0.0450 0.9445

6000 3.1416 0.0102 0.0311 0.0317 0.9450

#### **B.4 Simulation Results for DML**


An alternative to the semi-parametric two-step estimation of the welfare functional is to use

the double-debiased machine learning approach. The DML estimator is implemented via a

cross-fitting scheme. Specifically, the sample is partitioned into K folds. For each fold, we

use the observations in the remaining _K −_ 1 folds to obtain nonparametric estimates of the

nuisance functions _µ_ 0 ( _x,_ 1) and _µ_ 0 ( _x,_ 0) (in our case, using GAM). These estimates are then
applied to the held-out fold to take an average of the fitted indicator 1 _{µ_ ˆ 0 ( _x,_ 1) _−_ _µ_ ˆ 0 ( _x,_ 0) _≥_


36


0 _}_ . This process is repeated so that each fold serves once as the testing set, and the final

estimator is obtained by averaging the resulting values across all folds. A confidence interval

could then be constructed as in Theorem 2 with the debiased ML estimate in replacement

of the semi-parametric two-step estimate.

We simulate 1,000 iterations for Models 1 to 3 with _K_ = 5, and the results are summarized

in table 12. The confidence intervals exhibit slight undercoverage in small samples, but this

issue diminishes as the sample size increases. This pattern is consistent with the asymptotic

nature of the interval’s construction.


Table 12: Theorem DML Simulation Results


**Model** **n** _W_ 0 **Bias** **SD** **SE** **Coverage**


Model 1 1,500 0.3857 -0.01271 0.04583 0.04215 0.906
Model 1 3,000 0.3857 -0.00776 0.03184 0.02969 0.921
Model 1 6,000 0.3857 -0.00392 0.02193 0.02092 0.934


Model 2 1,500 0.2358 -0.01051 0.04900 0.04372 0.916
Model 2 3,000 0.2358 -0.00475 0.03319 0.03097 0.934
Model 2 6,000 0.2358 -0.00195 0.02297 0.02200 0.929


Model 3 1,500 0.5001 -0.00568 0.05636 0.05134 0.913
Model 3 3,000 0.5001 -0.00261 0.03701 0.03654 0.953
Model 3 6,000 0.5001 -0.00174 0.02628 0.02597 0.954

#### **B.5 Sieve Variance Estimation**

### **C Additional Results for Empirical Application**

#### **C.1 Sensitivity Analysis for Tuning Parameters**


When conducting inference for the value functional in the empirical analysis of the JTPA

data, two tuning parameters must be specified: _ι_ and _s_ . The parameter _ι_ determines the
size of the set _{x ∈_ _χ_ : _−ϵ <_ _h_ [ˆ] ( _x_ ) _< ϵ}_ . The parameter _s_ controls the smoothness of

the estimated density function over the trimmed dataset by scaling the bandwidths used in

kernel density estimation.

To evaluate robustness, we perform sensitivity analyses with respect to both tuning

parameters. The results, reported in Table 18, show that the estimated value functional

~~ˆ~~
_V_ (ˆ _µ_ ) is largely insensitive to the choice of _ι_, but more responsive to the choice of _s_ . This

shows the importance of choosing an appropriate kernel density estimator.


37


Table 13: Theorem 1 Simulation Results for Models 1–3 (Sieve Variance)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M1 1500 0.3857 0.0152 0.0469 0.0467 0.0035 0.9370

3000 0.3857 0.0076 0.0337 0.0329 0.0019 0.9390

6000 0.3857 0.0036 0.0229 0.0232 0.0011 0.9510


M2 1500 0.2358 0.0039 0.0481 0.0488 0.0029 0.9480

3000 0.2358 0.0042 0.0350 0.0345 0.0014 0.9425

6000 0.2358 0.0025 0.0241 0.0245 0.0007 0.9535


M3 1500 0.5001 0.0038 0.0580 0.0581 0.0023 0.9560

3000 0.5001 0.0019 0.0407 0.0413 0.0012 0.9560

6000 0.5001 -0.0002 0.0293 0.0294 0.0006 0.9525


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


Table 14: Theorem 1 Simulation Results for Models 4–7 (Sieve Variance)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M4 1500 0.1033 0.0185 0.0478 0.0482 0.0088 0.9400

3000 0.1033 0.0088 0.0348 0.0350 0.0048 0.9445

6000 0.1033 0.0043 0.0248 0.0251 0.0024 0.9490


M5 1500 0.0499 0.0282 0.0418 0.0431 0.0110 0.9310

3000 0.0499 0.0158 0.0303 0.0311 0.0068 0.9335

6000 0.0499 0.0094 0.0227 0.0224 0.0038 0.9290


M6 1500 0.2315 0.0268 0.0559 0.0552 0.0057 0.9260

3000 0.2315 0.0123 0.0394 0.0402 0.0030 0.9480

6000 0.2315 0.0050 0.0288 0.0288 0.0015 0.9490


M7 1500 0.1250 0.0462 0.0505 0.0502 0.0087 0.8920

3000 0.1250 0.0218 0.0346 0.0350 0.0046 0.9270

6000 0.1250 0.0101 0.0245 0.0245 0.0023 0.9400


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


38


Table 15: Theorem 2 Simulation Results for Models 8–10 (Sieve Variance)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M8 1500 0.3857 0.0068 0.0414 0.0417 0.0026 0.9475

3000 0.3857 0.0029 0.0296 0.0294 0.0014 0.9505

6000 0.3857 0.0006 0.0206 0.0208 0.0007 0.9555


M9 1500 0.2358 0.0042 0.0425 0.0431 0.0026 0.9555

3000 0.2358 0.0029 0.0304 0.0305 0.0012 0.9475

6000 0.2358 0.0012 0.0209 0.0216 0.0006 0.9510


M10 1500 0.5001 0.0067 0.0511 0.0511 0.0020 0.9480

3000 0.5001 0.0037 0.0357 0.0364 0.0011 0.9590

6000 0.5001 0.0013 0.0253 0.0259 0.0006 0.9580


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


Table 16: Theorem 2 Simulation Results for Models 11–14 (Sieve Variance)


Model _n_ _W_ true Bias SD SE SD(SE) Coverage


M11 1500 0.1033 0.0307 0.0365 0.0379 0.0041 0.9065

3000 0.1033 0.0156 0.0264 0.0272 0.0023 0.9290

6000 0.1033 0.0084 0.0192 0.0195 0.0013 0.9395


M12 1500 0.0499 0.0418 0.0316 0.0344 0.0050 0.8410

3000 0.0499 0.0232 0.0229 0.0245 0.0031 0.8930

6000 0.0499 0.0126 0.0168 0.0175 0.0018 0.9205


M13 1500 0.2315 0.0177 0.0432 0.0438 0.0030 0.9420

3000 0.2315 0.0088 0.0309 0.0313 0.0016 0.9440

6000 0.2315 0.0041 0.0221 0.0222 0.0008 0.9495


M14 1500 0.1250 0.0251 0.0382 0.0386 0.0050 0.9230

3000 0.1250 0.0117 0.0258 0.0268 0.0024 0.9440

6000 0.1250 0.0054 0.0182 0.0188 0.0012 0.9415


_Notes:_ (1) _W_ true denotes the true welfare functional, computed numerically using 5 _,_ 000 Sobol points drawn
from _F_ . (2) Bias is the average deviation from _W_ true . (3) SD is the sampling standard deviation. (4) SE is
the average estimated asymptotic standard error of _W_ ~~[ˆ]~~ (ˆ _µ_ ). (5) SD(SE) is the standard deviation of SE
across iterations. (6) Coverage is the empirical coverage probability of the 95% CI.


39


Table 17: Estimated Welfare Gains and Share of Population to be Treated Under Nonparametric Plug-in Rule (Sieve Variance)


(a) 30-Month Post-Program Earnings, No Treatment Cost


**Method** **Share Treated** **Est. Welfare Gain**



0.89 $1,519


(0.73 1.05) ($691, $2347)



0.89
Ours



($691, $2347)



0.91 $1,693


NA NA



0.91
KT (2018)



NA



(b) 30-Month Post-Program Earnings, $774 Cost per Treatment


**Method** **Share Treated** **Est. Welfare Gain**



0.80 $858


(0.53, 1.07)



0.80
Ours



($84, $1631)



0.78 $996


NA NA



0.78
KT (2018)



NA



_Note:_ Two-sided 95% confidence intervals in parentheses, constructed based on the asymptotic
distributions of the corresponding estimators.


40


Table 18: Sensitivity of _V_ ~~[ˆ]~~ (ˆ _µ_ ) to Tuning Parameters _ι_ and _s_ (Cost = F vs. Cost = T)


(a) Sensitivity to _ι_


ˆ
_ι_ _V_ SE CI Low CI High Num


**Cost = F**


0.005 0.8908 0.0866 0.7210 1.0606 475

0.0075 0.8908 0.0862 0.7219 1.0597 713

0.0100 0.8908 0.0828 0.7286 1.0530 931

0.0125 0.8908 0.0840 0.7262 1.0554 1203

0.0150 0.8908 0.0857 0.7228 1.0588 1463


**Cost = T**


0.005 0.7971 0.1343 0.5338 1.0603 689

0.0075 0.7971 0.1303 0.5417 1.0525 1044

0.0100 0.7971 0.1373 0.5279 1.0662 1411

0.0125 0.7971 0.1399 0.5229 1.0712 1784

0.0150 0.7971 0.1406 0.5214 1.0727 2152


(b) Sensitivity to _s_


ˆ
_s_ _V_ SE CI Low CI High Num


**Cost = F**


1 0.8908 0.0896 0.7151 1.0665 931

2 0.8908 0.0860 0.7224 1.0593 931

3 0.8908 0.0828 0.7286 1.0530 931

4 0.8908 0.0809 0.7322 1.0494 931

5 0.8908 0.0793 0.7354 1.0462 931


**Cost = T**


1 0.7971 0.1103 0.5809 1.0132 1411

2 0.7971 0.1254 0.5513 1.0428 1411

3 0.7971 0.1373 0.5279 1.0662 1411

4 0.7971 0.1420 0.5187 1.0754 1411

5 0.7971 0.1554 0.4925 1.1016 1411


_Notes:_ (a) Cost = T if the outcome variable is the 30-month earnings minus an additional $774
and F otherwise. (b) SE = ˆ _σ_ _V_ _/_ _[√]_ ~~_n_~~ ~~.~~ (c) _ϵ_ = _ι ×_ SD( _h_ [ˆ] ( _x_ )) over **X_trim** . (d)
CI Low = _V_ ~~[ˆ]~~ (ˆ _µ_ ) _−_ 1 _._ 96 _×_ SE, CI High = _V_ ~~[ˆ]~~ (ˆ _µ_ ) + 1 _._ 96 _×_ SE. (e) Num is the number of Sobol points
whose _h_ [ˆ] evaluations fall into _{x ∈_ **X_trim** : _|h_ [ˆ] ( _x_ ) _| < ϵ}_ .


41


#### **C.2 Sensitivity Analysis for Density Estimation**

We employed a naive Gaussian kernel density estimator for the covariate distribution in

the trimmed dataset, treating years of education as a continuous variable. While years

of education is theoretically continuous, in practice it takes on only discrete values in the

dataset. This motivates considering alternative density estimators that explicitly treat years

of education as categorical in order to assess the robustness of our inference. To this end, we

partitioned the trimmed dataset by educational level and examined three cases: (1) a naive

Gaussian kernel density estimator with bandwidths selected by `Hscv()` and scaled by a small

factor to ensure smoothness; (2) a logspline density estimator (Stone, Hansen, Kooperberg
and Truong (1997)); (3) a penalized B-spline density estimator (Schellhase and Kauermann
(2012)).

The results are presented in Table 19. As shown, when costs are taken into account, the

Gaussian kernel density estimator discussed in the main text yields relatively conservative

confidence intervals compared to the three alternatives. In contrast, when costs are not

considered, the results remain largely unchanged across all the estimators. However, we

should emphasize that these three alternative density estimators are applied based on the

assumption that years of education is categorical, but in theory it is treated as a continuous

variable, which provides justification for using the Gaussian kernel estimator in the main

text.


Table 19: Sensitivity of _V_ ~~[ˆ]~~ (ˆ _µ_ ) to Alternative Density Estimators


**Cost = T** **Cost = F**


Estimator _V_ ˆ SE CI Low CI High _V_ ˆ SE CI Low CI High


Logspline 0.7971 0.0932 0.6144 0.9797 0.8908 0.1041 0.6868 1.0948
Kernel 0.7971 0.0704 0.6590 0.9351 0.8908 0.0980 0.6988 1.0829

Penalized B-spline 0.7971 0.0601 0.6793 0.9148 0.8908 0.0691 0.7555 1.0262


_Notes:_ (a) Cost = T if the outcome variable is 30-month earnings minus an additional $774, and Cost = F
otherwise. (b) SE = ˆ _σ_ _V_ _/_ _[√]_ ~~_n_~~ ~~.~~ (c) CI Low = _V_ ~~[ˆ]~~ (ˆ _µ_ ) _−_ 1 _._ 96 _×_ SE, CI High = _V_ ~~[ˆ]~~ (ˆ _µ_ ) + 1 _._ 96 _×_ SE.

#### **C.3 Extrapolating Sieve Estimates**


We also examine the case where the dataset is not trimmed. In this setting, interpolation is

required to evaluate the estimated functions ˆ _µ_ ( _x,_ 0) and ˆ _µ_ ( _x,_ 1) outside the supports of the

control and treated groups, respectively. The formulas for the welfare and value functional

estimators, as well as their asymptotic variances, remain unchanged. The only difference is


42


that the sample averages are now computed over the full covariate support rather than the

trimmed dataset. The results for this specification are reported in Table 20.


Table 20: Estimated Welfare Gains and Share of Population to be Treated Under Nonparametric Plug-in Rule Without Trimming


(a) 30-Month Post-Program Earnings, No Treatment Cost


**Method** **Share Treated** **Est. Welfare Gain**



0.92 $1,459


(0.80 1.03) ($825, $2093)



0.92
Ours



($825, $2093)



0.91 $1,693


NA NA



0.91
KT (2018)



NA



(b) 30-Month Post-Program Earnings, $774 Cost per Treatment


**Method** **Share Treated** **Est. Welfare Gain**



0.85 $768


(0.71, 0.99)



0.85
Ours



($164, $1373)



0.78 $996


NA NA



0.78
KT (2018)



NA



_Note:_ Two-sided 95% confidence intervals in parentheses, constructed based on the asymptotic
distributions of the corresponding estimators.

#### C.4 Estimating Ω [ˆ] Using the Trimmed Dataset


We used the full sample to estimate the asymptotic variance-covariance matrix Ω. [ˆ] For
comparison, Table 21 reports the results obtained when Ωis instead estimated using only [ˆ]

the trimmed dataset. We observe that the CIs expands by a small factor.

#### **C.5 The Sieve Score Bootstrapping Procedure for Critical Values**


Following Chen and Christensen (2018), one could also use the sieve score bootstrap procedure to calculate the critical value used in the construction of the 95% CI for _V_ ( _µ_ 0 ).
The algorithm starts with making iid draws _{ω_ _i_ _}_ _[n]_ _i_ =1 [from a distribution independent of the]
data **df** with mean zero, unit variance, and finite third moment (e.g. a standard normal


43


Table 21: Estimated Share of Population to be Treated Under Nonparametric Plug-in Rule
With ΩEstimated Using Trimmed Data [ˆ]


(a) 30-Month Post-Program Earnings, No Treatment Cost


**Method** **Share Treated**


0.89
Ours

(0.72, 1.06)


0.91
KT (2018)

NA


(b) 30-Month Post-Program Earnings, $774 Cost per Treatment


**Method** **Share Treated**


0.80
Ours

(0.44, 1.15)


0.78
KT (2018)

NA


distribution), and then calculates the the bootstrap sieve t-statistic using the formula




[(] _[µ]_ [ˆ][)[] _[ν]_ []] _[′]_ [(] _[B]_ _[′]_ _[B][/][n]_ [)]
_Z_ _n_ _[∗]_ [=] _[DV]_ ˆ

_σ_ _V_ _/_ ~~_[√]_~~ ~~_n_~~







 ~~_√_~~ 1 ~~_n_~~ � _ni_ =1 1 _[ψ]_ _[K]_ [1] [(] _[x]_ _[i]_ [)ˆ] _[u]_ _[i]_ _[ω]_ _[i]_

1 _n_

 ~~_√n_~~ � _j_ = _n_ 1 +1 _[ψ]_ _[K]_ [0] [(] _[x]_ _[j]_ [)ˆ] _[u]_ _[j]_



1 _n_
~~_√n_~~ � _j_ = _n_ 1 +1 _[ψ]_ _[K]_ [0] [(] _[x]_ _[j]_ [)ˆ] _[u]_ _[j]_ _[ω]_ _[j]_









with _B_ being the design matrix of regression _Y_ _i_ on _D_ _i_ _ψ_ _[K]_ [1] ( _X_ _i_ ) and (1 _−D_ _i_ ) _ψ_ _[K]_ [0] ( _X_ _i_ ). The 95%
quantile of the bootstrapped _|Z_ _n_ _[∗]_ _[|]_ [ could be used as the critical value in the construction of]

the CI. Setting the number of bootstrap equals 1000, the resulting critical values is 1.865758,

not significantly different from 1.96.


44


